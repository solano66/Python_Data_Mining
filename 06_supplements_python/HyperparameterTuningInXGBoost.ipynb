{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning in XGBoost\n",
    "\n",
    "Load the dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>356059</td>\n",
       "      <td>0</td>\n",
       "      <td>27989</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>56.713568</td>\n",
       "      <td>42.0</td>\n",
       "      <td>57.458776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27700</th>\n",
       "      <td>5365996</td>\n",
       "      <td>40729</td>\n",
       "      <td>102442</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>60.612150</td>\n",
       "      <td>33.0</td>\n",
       "      <td>90.050007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16740</th>\n",
       "      <td>309999</td>\n",
       "      <td>28</td>\n",
       "      <td>43647</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>73.075949</td>\n",
       "      <td>45.0</td>\n",
       "      <td>84.541769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29069</th>\n",
       "      <td>50469</td>\n",
       "      <td>21748</td>\n",
       "      <td>2456</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.820312</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.418203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4583</th>\n",
       "      <td>104037</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.748691</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.213291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1       2   3    4      5          6     7          8   \\\n",
       "5399    356059      0   27989   8  0.0  422.0  56.713568  42.0  57.458776   \n",
       "27700  5365996  40729  102442   9  0.0  740.0  60.612150  33.0  90.050007   \n",
       "16740   309999     28   43647   9  0.0  532.0  73.075949  45.0  84.541769   \n",
       "29069    50469  21748    2456  32  0.0   47.0   5.820312   2.5   8.418203   \n",
       "4583    104037      0      30  14  0.0  113.0   6.748691   4.0  11.213291   \n",
       "\n",
       "        9   ...  44  45  46  47  48  49  50  51  52  53  \n",
       "5399   0.0  ...   0   0   0   0   0   1   0   0   0   2  \n",
       "27700  0.0  ...   0   1   1   0   0   0   0   0   0   2  \n",
       "16740  0.0  ...   0   0   0   0   0   0   0   0   1   0  \n",
       "29069  0.0  ...   1   0   0   1   0   0   0   0   0   0  \n",
       "4583   0.0  ...   0   1   0   0   1   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = \"./facebook+comment+volume+dataset/Training/Features_Variant_1.csv\"\n",
    "df = pd.read_csv(file, header=None)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 40949 entries and 54 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset has {} entries and {} features\".format(*df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:,:52].values, df.loc[:,53].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep 90% of the dataset for training, and 10% (or a .1 part) for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data into DMatrices\n",
    "\n",
    "As mentioned before, in order to use the native API for XGBoost, we will first need to build DMatrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb # conda install xgboost --y\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE is 11.31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# \"Learn\" the mean from the training data\n",
    "mean_train = np.mean(y_train)\n",
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE is {:.2f}\".format(mae_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Tuning an XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The params dictionary\n",
    "\n",
    "Let’s define it with default values for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters** num_boost_round **and** early_stopping_rounds\n",
    "\n",
    "The first parameter we will look at is not part of the params dictionary, but will be passed as a standalone argument to the training method. This parameter is called num_boost_round and corresponds to the number of boosting rounds or trees to build. Its optimal value highly depends on the other parameters, and thus it should be re-tuned each time you update a parameter. You could do this by tuning it together with all parameters in a grid-search, but it requires a lot of computational effort.\n",
    "\n",
    "To do so, we define a test dataset and a metric that is used to assess performance at each round. If performance haven’t improved for N rounds (N is defined by the variable early_stopping_round), we stop the training and keep the best number of boosting rounds. Let's see how to use it.\n",
    "\n",
    "First, we need to add the evaluation metric we are interested in to our params dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eval_metric'] = \"mae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to pass a num_boost_round which corresponds to the maximum number of boosting rounds that we allow. We set it to a large value hoping to find the optimal number of rounds before reaching it, if we haven't improved performance on our test dataset in early_stopping_round rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automatically find the best number of boosting rounds, we need to pass extra parameters on top of the params dictionary, the training DMatrix and num_boost_round:\n",
    "\n",
    "- evals: a list of pairs (test_dmatrix, name_of_test). Here we will use our dtest DMatrix.\n",
    "- early_stopping_rounds: The number of rounds without improvements after which we should stop, here we set it to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:8.58568\n",
      "[1]\tTest-mae:6.96160\n",
      "[2]\tTest-mae:5.86780\n",
      "[3]\tTest-mae:5.29799\n",
      "[4]\tTest-mae:4.91546\n",
      "[5]\tTest-mae:4.67942\n",
      "[6]\tTest-mae:4.51087\n",
      "[7]\tTest-mae:4.43763\n",
      "[8]\tTest-mae:4.37738\n",
      "[9]\tTest-mae:4.32590\n",
      "[10]\tTest-mae:4.28929\n",
      "[11]\tTest-mae:4.24682\n",
      "[12]\tTest-mae:4.20685\n",
      "[13]\tTest-mae:4.18984\n",
      "[14]\tTest-mae:4.16802\n",
      "[15]\tTest-mae:4.14928\n",
      "[16]\tTest-mae:4.13792\n",
      "[17]\tTest-mae:4.12976\n",
      "[18]\tTest-mae:4.12585\n",
      "[19]\tTest-mae:4.12675\n",
      "[20]\tTest-mae:4.12793\n",
      "[21]\tTest-mae:4.11582\n",
      "[22]\tTest-mae:4.11287\n",
      "[23]\tTest-mae:4.12059\n",
      "[24]\tTest-mae:4.11311\n",
      "[25]\tTest-mae:4.10039\n",
      "[26]\tTest-mae:4.10567\n",
      "[27]\tTest-mae:4.11535\n",
      "[28]\tTest-mae:4.11051\n",
      "[29]\tTest-mae:4.11479\n",
      "[30]\tTest-mae:4.11239\n",
      "[31]\tTest-mae:4.13754\n",
      "[32]\tTest-mae:4.13697\n",
      "[33]\tTest-mae:4.12812\n",
      "[34]\tTest-mae:4.14731\n",
      "[35]\tTest-mae:4.14775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:44] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: 4.10 with 26 rounds\n"
     ]
    }
   ],
   "source": [
    "print(\"Best MAE: {:.2f} with {} rounds\".format(\n",
    "                 model.best_score,\n",
    "                 model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we stopped before reaching the maximum number of boosting rounds, that’s because after the 26th tree, adding more rounds did not lead to improvements of MAE on the test dataset.\n",
    "Let’s keep this MAE in mind for later, this is the MAE of our model with default parameters and an optimal number of boosting rounds, on the test dataset. As you can see, we are already beating the baseline MAE 11.31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XGBoost’s CV\n",
    "\n",
    "In order to tune the other hyperparameters, we will use the cv function from XGBoost. It allows us to run cross-validation on our training dataset and returns a mean MAE score.\n",
    "\n",
    "We need to pass it:\n",
    "\n",
    "- params: our dictionary of parameters.\n",
    "- our dtrain matrix.\n",
    "- num_boost_round: number of boosting rounds. Here we will use a large number again and count on early_stopping_rounds to find the optimal number of rounds before reaching the maximum.\n",
    "- seed: random seed. It's important to set a seed here, to ensure we are using the same folds for each step so we can properly compare the scores with different parameters.\n",
    "- nfold: the number of folds to use for cross-validation\n",
    "- metrics: the metrics to use to evaluate our model, here we use MAE.\n",
    "\n",
    "As you can see, we don’t need to pass a test dataset here. It’s because the cross-validation function is splitting the train dataset into nfolds and iteratively keeps one of the folds for test purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-mae-mean</th>\n",
       "      <th>train-mae-std</th>\n",
       "      <th>test-mae-mean</th>\n",
       "      <th>test-mae-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.288764</td>\n",
       "      <td>0.117090</td>\n",
       "      <td>8.372614</td>\n",
       "      <td>0.236231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.563341</td>\n",
       "      <td>0.106052</td>\n",
       "      <td>6.793640</td>\n",
       "      <td>0.256606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.415179</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>5.822265</td>\n",
       "      <td>0.250534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.643518</td>\n",
       "      <td>0.057167</td>\n",
       "      <td>5.193959</td>\n",
       "      <td>0.247316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.131201</td>\n",
       "      <td>0.051492</td>\n",
       "      <td>4.823537</td>\n",
       "      <td>0.227656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.776277</td>\n",
       "      <td>0.042514</td>\n",
       "      <td>4.578252</td>\n",
       "      <td>0.217193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.534382</td>\n",
       "      <td>0.038187</td>\n",
       "      <td>4.442351</td>\n",
       "      <td>0.218526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.356895</td>\n",
       "      <td>0.032615</td>\n",
       "      <td>4.347466</td>\n",
       "      <td>0.217143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.218539</td>\n",
       "      <td>0.031009</td>\n",
       "      <td>4.298237</td>\n",
       "      <td>0.218510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.117893</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>4.260752</td>\n",
       "      <td>0.216624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.043007</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>4.228298</td>\n",
       "      <td>0.207904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.986239</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>4.200739</td>\n",
       "      <td>0.199768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.931240</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>4.180739</td>\n",
       "      <td>0.199653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.876211</td>\n",
       "      <td>0.033987</td>\n",
       "      <td>4.161365</td>\n",
       "      <td>0.192140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.838039</td>\n",
       "      <td>0.032611</td>\n",
       "      <td>4.156653</td>\n",
       "      <td>0.200453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.789256</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>4.147822</td>\n",
       "      <td>0.196088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.757240</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>4.141522</td>\n",
       "      <td>0.200565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.720425</td>\n",
       "      <td>0.036964</td>\n",
       "      <td>4.134239</td>\n",
       "      <td>0.203032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.691348</td>\n",
       "      <td>0.041032</td>\n",
       "      <td>4.128264</td>\n",
       "      <td>0.199610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.662498</td>\n",
       "      <td>0.046535</td>\n",
       "      <td>4.131945</td>\n",
       "      <td>0.197635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.633047</td>\n",
       "      <td>0.044264</td>\n",
       "      <td>4.126031</td>\n",
       "      <td>0.201401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.607465</td>\n",
       "      <td>0.045078</td>\n",
       "      <td>4.122296</td>\n",
       "      <td>0.193584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.574352</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>4.116059</td>\n",
       "      <td>0.196195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.551092</td>\n",
       "      <td>0.045104</td>\n",
       "      <td>4.117202</td>\n",
       "      <td>0.197284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.524129</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>4.108327</td>\n",
       "      <td>0.198284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.490972</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>4.110748</td>\n",
       "      <td>0.200956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.473820</td>\n",
       "      <td>0.040546</td>\n",
       "      <td>4.108313</td>\n",
       "      <td>0.194517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.447877</td>\n",
       "      <td>0.040428</td>\n",
       "      <td>4.109427</td>\n",
       "      <td>0.194587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.422345</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>4.108951</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.402607</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>4.105201</td>\n",
       "      <td>0.186119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.381349</td>\n",
       "      <td>0.033517</td>\n",
       "      <td>4.108838</td>\n",
       "      <td>0.185290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.359189</td>\n",
       "      <td>0.030212</td>\n",
       "      <td>4.103320</td>\n",
       "      <td>0.185797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.339776</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>4.103521</td>\n",
       "      <td>0.186554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.325412</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>4.103060</td>\n",
       "      <td>0.184693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.303362</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>4.104417</td>\n",
       "      <td>0.187463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.284917</td>\n",
       "      <td>0.042341</td>\n",
       "      <td>4.103471</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.266569</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>4.102799</td>\n",
       "      <td>0.189977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.251610</td>\n",
       "      <td>0.040397</td>\n",
       "      <td>4.103856</td>\n",
       "      <td>0.186810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.235585</td>\n",
       "      <td>0.040356</td>\n",
       "      <td>4.103572</td>\n",
       "      <td>0.184730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.219041</td>\n",
       "      <td>0.042748</td>\n",
       "      <td>4.104013</td>\n",
       "      <td>0.181950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.201757</td>\n",
       "      <td>0.037584</td>\n",
       "      <td>4.104005</td>\n",
       "      <td>0.180875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.181583</td>\n",
       "      <td>0.040068</td>\n",
       "      <td>4.105051</td>\n",
       "      <td>0.180036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.165750</td>\n",
       "      <td>0.038385</td>\n",
       "      <td>4.104522</td>\n",
       "      <td>0.179961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.150768</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>4.103080</td>\n",
       "      <td>0.181055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.134375</td>\n",
       "      <td>0.039384</td>\n",
       "      <td>4.101022</td>\n",
       "      <td>0.183347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
       "0         8.288764       0.117090       8.372614      0.236231\n",
       "1         6.563341       0.106052       6.793640      0.256606\n",
       "2         5.415179       0.075973       5.822265      0.250534\n",
       "3         4.643518       0.057167       5.193959      0.247316\n",
       "4         4.131201       0.051492       4.823537      0.227656\n",
       "5         3.776277       0.042514       4.578252      0.217193\n",
       "6         3.534382       0.038187       4.442351      0.218526\n",
       "7         3.356895       0.032615       4.347466      0.217143\n",
       "8         3.218539       0.031009       4.298237      0.218510\n",
       "9         3.117893       0.033844       4.260752      0.216624\n",
       "10        3.043007       0.031082       4.228298      0.207904\n",
       "11        2.986239       0.034129       4.200739      0.199768\n",
       "12        2.931240       0.032275       4.180739      0.199653\n",
       "13        2.876211       0.033987       4.161365      0.192140\n",
       "14        2.838039       0.032611       4.156653      0.200453\n",
       "15        2.789256       0.029104       4.147822      0.196088\n",
       "16        2.757240       0.032888       4.141522      0.200565\n",
       "17        2.720425       0.036964       4.134239      0.203032\n",
       "18        2.691348       0.041032       4.128264      0.199610\n",
       "19        2.662498       0.046535       4.131945      0.197635\n",
       "20        2.633047       0.044264       4.126031      0.201401\n",
       "21        2.607465       0.045078       4.122296      0.193584\n",
       "22        2.574352       0.044651       4.116059      0.196195\n",
       "23        2.551092       0.045104       4.117202      0.197284\n",
       "24        2.524129       0.044828       4.108327      0.198284\n",
       "25        2.490972       0.036617       4.110748      0.200956\n",
       "26        2.473820       0.040546       4.108313      0.194517\n",
       "27        2.447877       0.040428       4.109427      0.194587\n",
       "28        2.422345       0.032829       4.108951      0.189700\n",
       "29        2.402607       0.032678       4.105201      0.186119\n",
       "30        2.381349       0.033517       4.108838      0.185290\n",
       "31        2.359189       0.030212       4.103320      0.185797\n",
       "32        2.339776       0.033195       4.103521      0.186554\n",
       "33        2.325412       0.038189       4.103060      0.184693\n",
       "34        2.303362       0.038728       4.104417      0.187463\n",
       "35        2.284917       0.042341       4.103471      0.186600\n",
       "36        2.266569       0.040833       4.102799      0.189977\n",
       "37        2.251610       0.040397       4.103856      0.186810\n",
       "38        2.235585       0.040356       4.103572      0.184730\n",
       "39        2.219041       0.042748       4.104013      0.181950\n",
       "40        2.201757       0.037584       4.104005      0.180875\n",
       "41        2.181583       0.040068       4.105051      0.180036\n",
       "42        2.165750       0.038385       4.104522      0.179961\n",
       "43        2.150768       0.037299       4.103080      0.181055\n",
       "44        2.134375       0.039384       4.101022      0.183347"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'mae'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_boost_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv returns a table where the rows correspond to the number of boosting trees used, here again, we stopped before the 999 rounds (fortunately!).\n",
    "\n",
    "The 4 columns correspond to the mean and standard deviation of MAE on the test dataset and on the train dataset. For this tutorial we will only try to improve the mean test MAE. We can get the best MAE score from cv with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1010219056723916"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test-mae-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to use cv, we are ready to start tuning! We will first tune our parameters to minimize the MAE on cross-validation, and then check the performance of our model on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters** max_depth **and** min_child_weight\n",
    "\n",
    "Those parameters add constraints on the architecture of the trees.\n",
    "\n",
    "- max_depth is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships by adding more nodes, but as we go deeper, splits become less relevant and are sometimes only due to noise, causing the model to overfit.\n",
    "\n",
    "- min_child_weight is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n",
    "\n",
    "Thus, those parameters can be used to control the complexity of the trees. It is important to tune them together in order to find a good trade-off between model bias and variance\n",
    "\n",
    "Let’s make a list containing all the combinations max_depth/min_child_weight that we want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try wider intervals with a larger step between\n",
    "# each value and then narrow it down. Here after several\n",
    "# iteration I found that the optimal value was in the\n",
    "# following ranges.\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "] # 9 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run cross validation on each of those pairs. It can take some time…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:45] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.188793890644826 for 19 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:46] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.191833542466357 for 11 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tMAE 4.212599884308445 for 12 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tMAE 4.210564874878322 for 16 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:47] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.230052083374737 for 16 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:48] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.222578605588366 for 12 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tMAE 4.189796710935704 for 14 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:49] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.212010991899288 for 13 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tMAE 4.210864445213987 for 14 rounds\n",
      "Best params: 9, 5, MAE: 4.188793890644826\n"
     ]
    }
   ],
   "source": [
    "# Define initial best params and MAE\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best score with a max_depth of 9 and min_child_weight of 5, so let's update our params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 9\n",
    "params['min_child_weight'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters** subsample **and** colsample_bytree\n",
    "\n",
    "Those parameters control the sampling of the dataset that is done at each boosting round.\n",
    "\n",
    "Instead of using the whole training set every time, we can build a tree on slightly different data at each step, which makes it less likely to overfit to a single sample or feature.\n",
    "\n",
    "- subsample corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n",
    "- colsample_bytree corresponds to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features.\n",
    "\n",
    "Let’s see if we can get better results by tuning those parameters together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "] # 16 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.7), (0.7, 0.8), (0.7, 0.9), (0.7, 1.0), (0.8, 0.7), (0.8, 0.8), (0.8, 0.9), (0.8, 1.0), (0.9, 0.7), (0.9, 0.8), (0.9, 0.9), (0.9, 1.0), (1.0, 0.7), (1.0, 0.8), (1.0, 0.9), (1.0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(gridsearch_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can take some time…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:50] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.230052083374737 for 16 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:51] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.327802396522925 for 12 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tMAE 4.46603263111348 for 17 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:52] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.67037331882947 for 12 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tMAE 4.252509337708058 for 14 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:53] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.286404976346348 for 13 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tMAE 4.475591388381757 for 12 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:54] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.612627758676085 for 12 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tMAE 4.210677791347896 for 10 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tMAE 4.334051620788318 for 11 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:55] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.454363277746388 for 12 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tMAE 4.619639843454797 for 12 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:56] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.231604512230874 for 11 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tMAE 4.319851888425975 for 11 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:57] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMAE 4.489076813622117 for 12 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tMAE 4.689236656114426 for 12 rounds\n",
      "Best params: 0.8, 1.0, MAE: 4.210677791347896\n"
     ]
    }
   ],
   "source": [
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we update our params dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = .8\n",
    "params['colsample_bytree'] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter** ETA\n",
    "\n",
    "The ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step (remember how each boosting round is correcting the errors of the previous?).\n",
    "\n",
    "In practice, having a lower eta makes our model more robust to overfitting thus, usually, the lower the learning rate, the best. But with a lower eta, we need more boosting rounds, which takes more time to train, sometimes for only marginal improvements. Let's try a couple of values here, and time them with the notebook command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 19 µs, total: 28 µs\n",
      "Wall time: 3.81 µs\n",
      "CV with eta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:58] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 s, sys: 2.3 s, total: 5.6 s\n",
      "Wall time: 423 ms\n",
      "\tMAE 4.210677791347896 for 10 rounds\n",
      "\n",
      "CV with eta=0.2\n",
      "CPU times: user 4.75 s, sys: 3.14 s, total: 7.89 s\n",
      "Wall time: 590 ms\n",
      "\tMAE 4.104137217788904 for 21 rounds\n",
      "\n",
      "CV with eta=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:58:59] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.9 s, sys: 5.06 s, total: 13 s\n",
      "Wall time: 1 s\n",
      "\tMAE 3.998718575226202 for 44 rounds\n",
      "\n",
      "CV with eta=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:00] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 9.85 s, total: 25.1 s\n",
      "Wall time: 1.88 s\n",
      "\tMAE 3.9605776315872774 for 100 rounds\n",
      "\n",
      "CV with eta=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:01] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 46.3 s, total: 1min 56s\n",
      "Wall time: 8.78 s\n",
      "\tMAE 3.905728601651572 for 509 rounds\n",
      "\n",
      "CV with eta=0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:10] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 1min 28s, total: 3min 43s\n",
      "Wall time: 16.9 s\n",
      "\tMAE 3.907620621708253 for 993 rounds\n",
      "\n",
      "Best params: 0.01, MAE: 3.905728601651572\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# This can take some time…\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    # Run and time CV\n",
    "    %time cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['mae'], early_stopping_rounds=10)\n",
    "    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Here is how our final dictionary of parameters looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'min_child_weight': 6,\n",
       " 'eta': 0.01,\n",
       " 'subsample': 0.8,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'objective': 'reg:linear',\n",
       " 'eval_metric': 'mae'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train a model with it and see how well it does on our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:11.20560\n",
      "[1]\tTest-mae:11.10354\n",
      "[2]\tTest-mae:11.00289\n",
      "[3]\tTest-mae:10.90358\n",
      "[4]\tTest-mae:10.80749\n",
      "[5]\tTest-mae:10.70781\n",
      "[6]\tTest-mae:10.60834\n",
      "[7]\tTest-mae:10.51156\n",
      "[8]\tTest-mae:10.41670\n",
      "[9]\tTest-mae:10.32266\n",
      "[10]\tTest-mae:10.23176\n",
      "[11]\tTest-mae:10.14105\n",
      "[12]\tTest-mae:10.05041\n",
      "[13]\tTest-mae:9.96023\n",
      "[14]\tTest-mae:9.87409\n",
      "[15]\tTest-mae:9.78696\n",
      "[16]\tTest-mae:9.70213\n",
      "[17]\tTest-mae:9.62004\n",
      "[18]\tTest-mae:9.53587\n",
      "[19]\tTest-mae:9.45630\n",
      "[20]\tTest-mae:9.37353\n",
      "[21]\tTest-mae:9.29651\n",
      "[22]\tTest-mae:9.21901\n",
      "[23]\tTest-mae:9.14052\n",
      "[24]\tTest-mae:9.06745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:27] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\tTest-mae:8.99583\n",
      "[26]\tTest-mae:8.92462\n",
      "[27]\tTest-mae:8.85509\n",
      "[28]\tTest-mae:8.78713\n",
      "[29]\tTest-mae:8.72017\n",
      "[30]\tTest-mae:8.65116\n",
      "[31]\tTest-mae:8.58471\n",
      "[32]\tTest-mae:8.51927\n",
      "[33]\tTest-mae:8.45378\n",
      "[34]\tTest-mae:8.38722\n",
      "[35]\tTest-mae:8.32398\n",
      "[36]\tTest-mae:8.25836\n",
      "[37]\tTest-mae:8.19993\n",
      "[38]\tTest-mae:8.14226\n",
      "[39]\tTest-mae:8.08359\n",
      "[40]\tTest-mae:8.02478\n",
      "[41]\tTest-mae:7.96481\n",
      "[42]\tTest-mae:7.90802\n",
      "[43]\tTest-mae:7.85148\n",
      "[44]\tTest-mae:7.79544\n",
      "[45]\tTest-mae:7.73972\n",
      "[46]\tTest-mae:7.68544\n",
      "[47]\tTest-mae:7.63283\n",
      "[48]\tTest-mae:7.58040\n",
      "[49]\tTest-mae:7.52836\n",
      "[50]\tTest-mae:7.47415\n",
      "[51]\tTest-mae:7.42512\n",
      "[52]\tTest-mae:7.37515\n",
      "[53]\tTest-mae:7.32517\n",
      "[54]\tTest-mae:7.27579\n",
      "[55]\tTest-mae:7.22705\n",
      "[56]\tTest-mae:7.17807\n",
      "[57]\tTest-mae:7.13342\n",
      "[58]\tTest-mae:7.08749\n",
      "[59]\tTest-mae:7.04221\n",
      "[60]\tTest-mae:6.99550\n",
      "[61]\tTest-mae:6.94923\n",
      "[62]\tTest-mae:6.90776\n",
      "[63]\tTest-mae:6.86616\n",
      "[64]\tTest-mae:6.82330\n",
      "[65]\tTest-mae:6.78270\n",
      "[66]\tTest-mae:6.73928\n",
      "[67]\tTest-mae:6.69883\n",
      "[68]\tTest-mae:6.65647\n",
      "[69]\tTest-mae:6.61815\n",
      "[70]\tTest-mae:6.58137\n",
      "[71]\tTest-mae:6.54556\n",
      "[72]\tTest-mae:6.50974\n",
      "[73]\tTest-mae:6.47243\n",
      "[74]\tTest-mae:6.43474\n",
      "[75]\tTest-mae:6.39817\n",
      "[76]\tTest-mae:6.36201\n",
      "[77]\tTest-mae:6.32813\n",
      "[78]\tTest-mae:6.29388\n",
      "[79]\tTest-mae:6.26138\n",
      "[80]\tTest-mae:6.22771\n",
      "[81]\tTest-mae:6.19700\n",
      "[82]\tTest-mae:6.16595\n",
      "[83]\tTest-mae:6.13389\n",
      "[84]\tTest-mae:6.10070\n",
      "[85]\tTest-mae:6.07047\n",
      "[86]\tTest-mae:6.04090\n",
      "[87]\tTest-mae:6.00873\n",
      "[88]\tTest-mae:5.97738\n",
      "[89]\tTest-mae:5.94771\n",
      "[90]\tTest-mae:5.91897\n",
      "[91]\tTest-mae:5.88954\n",
      "[92]\tTest-mae:5.86048\n",
      "[93]\tTest-mae:5.83223\n",
      "[94]\tTest-mae:5.80675\n",
      "[95]\tTest-mae:5.77850\n",
      "[96]\tTest-mae:5.75106\n",
      "[97]\tTest-mae:5.72555\n",
      "[98]\tTest-mae:5.69836\n",
      "[99]\tTest-mae:5.67205\n",
      "[100]\tTest-mae:5.64620\n",
      "[101]\tTest-mae:5.62118\n",
      "[102]\tTest-mae:5.59898\n",
      "[103]\tTest-mae:5.57352\n",
      "[104]\tTest-mae:5.54785\n",
      "[105]\tTest-mae:5.52230\n",
      "[106]\tTest-mae:5.49721\n",
      "[107]\tTest-mae:5.47288\n",
      "[108]\tTest-mae:5.45012\n",
      "[109]\tTest-mae:5.42879\n",
      "[110]\tTest-mae:5.40863\n",
      "[111]\tTest-mae:5.38680\n",
      "[112]\tTest-mae:5.36564\n",
      "[113]\tTest-mae:5.34453\n",
      "[114]\tTest-mae:5.32340\n",
      "[115]\tTest-mae:5.30235\n",
      "[116]\tTest-mae:5.28209\n",
      "[117]\tTest-mae:5.26061\n",
      "[118]\tTest-mae:5.24083\n",
      "[119]\tTest-mae:5.22314\n",
      "[120]\tTest-mae:5.20296\n",
      "[121]\tTest-mae:5.18561\n",
      "[122]\tTest-mae:5.16384\n",
      "[123]\tTest-mae:5.14687\n",
      "[124]\tTest-mae:5.12900\n",
      "[125]\tTest-mae:5.11118\n",
      "[126]\tTest-mae:5.09245\n",
      "[127]\tTest-mae:5.07884\n",
      "[128]\tTest-mae:5.06039\n",
      "[129]\tTest-mae:5.04584\n",
      "[130]\tTest-mae:5.03120\n",
      "[131]\tTest-mae:5.01339\n",
      "[132]\tTest-mae:4.99617\n",
      "[133]\tTest-mae:4.98127\n",
      "[134]\tTest-mae:4.96698\n",
      "[135]\tTest-mae:4.95431\n",
      "[136]\tTest-mae:4.94109\n",
      "[137]\tTest-mae:4.92940\n",
      "[138]\tTest-mae:4.91580\n",
      "[139]\tTest-mae:4.90446\n",
      "[140]\tTest-mae:4.89030\n",
      "[141]\tTest-mae:4.87900\n",
      "[142]\tTest-mae:4.86567\n",
      "[143]\tTest-mae:4.85541\n",
      "[144]\tTest-mae:4.84464\n",
      "[145]\tTest-mae:4.83285\n",
      "[146]\tTest-mae:4.81967\n",
      "[147]\tTest-mae:4.80809\n",
      "[148]\tTest-mae:4.79548\n",
      "[149]\tTest-mae:4.78401\n",
      "[150]\tTest-mae:4.77270\n",
      "[151]\tTest-mae:4.76278\n",
      "[152]\tTest-mae:4.75288\n",
      "[153]\tTest-mae:4.74191\n",
      "[154]\tTest-mae:4.73426\n",
      "[155]\tTest-mae:4.72505\n",
      "[156]\tTest-mae:4.71488\n",
      "[157]\tTest-mae:4.70714\n",
      "[158]\tTest-mae:4.69726\n",
      "[159]\tTest-mae:4.68777\n",
      "[160]\tTest-mae:4.67886\n",
      "[161]\tTest-mae:4.66854\n",
      "[162]\tTest-mae:4.65988\n",
      "[163]\tTest-mae:4.65092\n",
      "[164]\tTest-mae:4.64035\n",
      "[165]\tTest-mae:4.62994\n",
      "[166]\tTest-mae:4.62232\n",
      "[167]\tTest-mae:4.61161\n",
      "[168]\tTest-mae:4.60422\n",
      "[169]\tTest-mae:4.59444\n",
      "[170]\tTest-mae:4.58616\n",
      "[171]\tTest-mae:4.57824\n",
      "[172]\tTest-mae:4.56940\n",
      "[173]\tTest-mae:4.56067\n",
      "[174]\tTest-mae:4.55371\n",
      "[175]\tTest-mae:4.54590\n",
      "[176]\tTest-mae:4.53760\n",
      "[177]\tTest-mae:4.53040\n",
      "[178]\tTest-mae:4.52195\n",
      "[179]\tTest-mae:4.51491\n",
      "[180]\tTest-mae:4.50829\n",
      "[181]\tTest-mae:4.50157\n",
      "[182]\tTest-mae:4.49299\n",
      "[183]\tTest-mae:4.48468\n",
      "[184]\tTest-mae:4.47711\n",
      "[185]\tTest-mae:4.47109\n",
      "[186]\tTest-mae:4.46426\n",
      "[187]\tTest-mae:4.45906\n",
      "[188]\tTest-mae:4.45273\n",
      "[189]\tTest-mae:4.44633\n",
      "[190]\tTest-mae:4.44097\n",
      "[191]\tTest-mae:4.43687\n",
      "[192]\tTest-mae:4.43264\n",
      "[193]\tTest-mae:4.42822\n",
      "[194]\tTest-mae:4.42128\n",
      "[195]\tTest-mae:4.41572\n",
      "[196]\tTest-mae:4.41065\n",
      "[197]\tTest-mae:4.40655\n",
      "[198]\tTest-mae:4.40207\n",
      "[199]\tTest-mae:4.39722\n",
      "[200]\tTest-mae:4.39152\n",
      "[201]\tTest-mae:4.38554\n",
      "[202]\tTest-mae:4.38088\n",
      "[203]\tTest-mae:4.37566\n",
      "[204]\tTest-mae:4.37048\n",
      "[205]\tTest-mae:4.36722\n",
      "[206]\tTest-mae:4.36360\n",
      "[207]\tTest-mae:4.35864\n",
      "[208]\tTest-mae:4.35402\n",
      "[209]\tTest-mae:4.34781\n",
      "[210]\tTest-mae:4.34222\n",
      "[211]\tTest-mae:4.33666\n",
      "[212]\tTest-mae:4.33353\n",
      "[213]\tTest-mae:4.32862\n",
      "[214]\tTest-mae:4.32405\n",
      "[215]\tTest-mae:4.31946\n",
      "[216]\tTest-mae:4.31468\n",
      "[217]\tTest-mae:4.30835\n",
      "[218]\tTest-mae:4.30346\n",
      "[219]\tTest-mae:4.29827\n",
      "[220]\tTest-mae:4.29403\n",
      "[221]\tTest-mae:4.28947\n",
      "[222]\tTest-mae:4.28619\n",
      "[223]\tTest-mae:4.28134\n",
      "[224]\tTest-mae:4.27793\n",
      "[225]\tTest-mae:4.27246\n",
      "[226]\tTest-mae:4.27005\n",
      "[227]\tTest-mae:4.26564\n",
      "[228]\tTest-mae:4.26104\n",
      "[229]\tTest-mae:4.25876\n",
      "[230]\tTest-mae:4.25650\n",
      "[231]\tTest-mae:4.25401\n",
      "[232]\tTest-mae:4.25195\n",
      "[233]\tTest-mae:4.24888\n",
      "[234]\tTest-mae:4.24630\n",
      "[235]\tTest-mae:4.24291\n",
      "[236]\tTest-mae:4.23889\n",
      "[237]\tTest-mae:4.23569\n",
      "[238]\tTest-mae:4.23348\n",
      "[239]\tTest-mae:4.23026\n",
      "[240]\tTest-mae:4.22786\n",
      "[241]\tTest-mae:4.22409\n",
      "[242]\tTest-mae:4.22143\n",
      "[243]\tTest-mae:4.21938\n",
      "[244]\tTest-mae:4.21616\n",
      "[245]\tTest-mae:4.21302\n",
      "[246]\tTest-mae:4.20939\n",
      "[247]\tTest-mae:4.20659\n",
      "[248]\tTest-mae:4.20284\n",
      "[249]\tTest-mae:4.20088\n",
      "[250]\tTest-mae:4.19750\n",
      "[251]\tTest-mae:4.19642\n",
      "[252]\tTest-mae:4.19382\n",
      "[253]\tTest-mae:4.19188\n",
      "[254]\tTest-mae:4.19042\n",
      "[255]\tTest-mae:4.18795\n",
      "[256]\tTest-mae:4.18540\n",
      "[257]\tTest-mae:4.18366\n",
      "[258]\tTest-mae:4.18244\n",
      "[259]\tTest-mae:4.18036\n",
      "[260]\tTest-mae:4.17927\n",
      "[261]\tTest-mae:4.17673\n",
      "[262]\tTest-mae:4.17461\n",
      "[263]\tTest-mae:4.17403\n",
      "[264]\tTest-mae:4.17153\n",
      "[265]\tTest-mae:4.16979\n",
      "[266]\tTest-mae:4.16866\n",
      "[267]\tTest-mae:4.16757\n",
      "[268]\tTest-mae:4.16535\n",
      "[269]\tTest-mae:4.16318\n",
      "[270]\tTest-mae:4.16127\n",
      "[271]\tTest-mae:4.15849\n",
      "[272]\tTest-mae:4.15700\n",
      "[273]\tTest-mae:4.15611\n",
      "[274]\tTest-mae:4.15417\n",
      "[275]\tTest-mae:4.15278\n",
      "[276]\tTest-mae:4.15108\n",
      "[277]\tTest-mae:4.14909\n",
      "[278]\tTest-mae:4.14736\n",
      "[279]\tTest-mae:4.14692\n",
      "[280]\tTest-mae:4.14584\n",
      "[281]\tTest-mae:4.14468\n",
      "[282]\tTest-mae:4.14203\n",
      "[283]\tTest-mae:4.14064\n",
      "[284]\tTest-mae:4.13957\n",
      "[285]\tTest-mae:4.13929\n",
      "[286]\tTest-mae:4.13617\n",
      "[287]\tTest-mae:4.13565\n",
      "[288]\tTest-mae:4.13310\n",
      "[289]\tTest-mae:4.13109\n",
      "[290]\tTest-mae:4.12905\n",
      "[291]\tTest-mae:4.12562\n",
      "[292]\tTest-mae:4.12374\n",
      "[293]\tTest-mae:4.12170\n",
      "[294]\tTest-mae:4.12147\n",
      "[295]\tTest-mae:4.11974\n",
      "[296]\tTest-mae:4.11878\n",
      "[297]\tTest-mae:4.11724\n",
      "[298]\tTest-mae:4.11500\n",
      "[299]\tTest-mae:4.11383\n",
      "[300]\tTest-mae:4.11106\n",
      "[301]\tTest-mae:4.10970\n",
      "[302]\tTest-mae:4.10927\n",
      "[303]\tTest-mae:4.10698\n",
      "[304]\tTest-mae:4.10535\n",
      "[305]\tTest-mae:4.10463\n",
      "[306]\tTest-mae:4.10389\n",
      "[307]\tTest-mae:4.10406\n",
      "[308]\tTest-mae:4.10290\n",
      "[309]\tTest-mae:4.10153\n",
      "[310]\tTest-mae:4.10084\n",
      "[311]\tTest-mae:4.10054\n",
      "[312]\tTest-mae:4.10015\n",
      "[313]\tTest-mae:4.09877\n",
      "[314]\tTest-mae:4.09594\n",
      "[315]\tTest-mae:4.09462\n",
      "[316]\tTest-mae:4.09254\n",
      "[317]\tTest-mae:4.09154\n",
      "[318]\tTest-mae:4.09173\n",
      "[319]\tTest-mae:4.09077\n",
      "[320]\tTest-mae:4.09074\n",
      "[321]\tTest-mae:4.09047\n",
      "[322]\tTest-mae:4.08936\n",
      "[323]\tTest-mae:4.08879\n",
      "[324]\tTest-mae:4.08878\n",
      "[325]\tTest-mae:4.08831\n",
      "[326]\tTest-mae:4.08654\n",
      "[327]\tTest-mae:4.08555\n",
      "[328]\tTest-mae:4.08452\n",
      "[329]\tTest-mae:4.08314\n",
      "[330]\tTest-mae:4.08204\n",
      "[331]\tTest-mae:4.08059\n",
      "[332]\tTest-mae:4.07939\n",
      "[333]\tTest-mae:4.07897\n",
      "[334]\tTest-mae:4.07779\n",
      "[335]\tTest-mae:4.07662\n",
      "[336]\tTest-mae:4.07397\n",
      "[337]\tTest-mae:4.07306\n",
      "[338]\tTest-mae:4.07178\n",
      "[339]\tTest-mae:4.07044\n",
      "[340]\tTest-mae:4.06975\n",
      "[341]\tTest-mae:4.06899\n",
      "[342]\tTest-mae:4.06806\n",
      "[343]\tTest-mae:4.06554\n",
      "[344]\tTest-mae:4.06514\n",
      "[345]\tTest-mae:4.06383\n",
      "[346]\tTest-mae:4.06235\n",
      "[347]\tTest-mae:4.06154\n",
      "[348]\tTest-mae:4.05998\n",
      "[349]\tTest-mae:4.05854\n",
      "[350]\tTest-mae:4.05681\n",
      "[351]\tTest-mae:4.05626\n",
      "[352]\tTest-mae:4.05554\n",
      "[353]\tTest-mae:4.05601\n",
      "[354]\tTest-mae:4.05522\n",
      "[355]\tTest-mae:4.05447\n",
      "[356]\tTest-mae:4.05369\n",
      "[357]\tTest-mae:4.05218\n",
      "[358]\tTest-mae:4.05155\n",
      "[359]\tTest-mae:4.05079\n",
      "[360]\tTest-mae:4.05010\n",
      "[361]\tTest-mae:4.04901\n",
      "[362]\tTest-mae:4.04892\n",
      "[363]\tTest-mae:4.04862\n",
      "[364]\tTest-mae:4.04812\n",
      "[365]\tTest-mae:4.04819\n",
      "[366]\tTest-mae:4.04775\n",
      "[367]\tTest-mae:4.04630\n",
      "[368]\tTest-mae:4.04438\n",
      "[369]\tTest-mae:4.04388\n",
      "[370]\tTest-mae:4.04395\n",
      "[371]\tTest-mae:4.04302\n",
      "[372]\tTest-mae:4.04216\n",
      "[373]\tTest-mae:4.04190\n",
      "[374]\tTest-mae:4.04135\n",
      "[375]\tTest-mae:4.03983\n",
      "[376]\tTest-mae:4.03963\n",
      "[377]\tTest-mae:4.03916\n",
      "[378]\tTest-mae:4.04069\n",
      "[379]\tTest-mae:4.03949\n",
      "[380]\tTest-mae:4.03859\n",
      "[381]\tTest-mae:4.03775\n",
      "[382]\tTest-mae:4.03688\n",
      "[383]\tTest-mae:4.03567\n",
      "[384]\tTest-mae:4.03475\n",
      "[385]\tTest-mae:4.03409\n",
      "[386]\tTest-mae:4.03237\n",
      "[387]\tTest-mae:4.03162\n",
      "[388]\tTest-mae:4.03116\n",
      "[389]\tTest-mae:4.03054\n",
      "[390]\tTest-mae:4.03023\n",
      "[391]\tTest-mae:4.02995\n",
      "[392]\tTest-mae:4.03035\n",
      "[393]\tTest-mae:4.02918\n",
      "[394]\tTest-mae:4.02839\n",
      "[395]\tTest-mae:4.02822\n",
      "[396]\tTest-mae:4.02827\n",
      "[397]\tTest-mae:4.02810\n",
      "[398]\tTest-mae:4.02773\n",
      "[399]\tTest-mae:4.02769\n",
      "[400]\tTest-mae:4.02669\n",
      "[401]\tTest-mae:4.02664\n",
      "[402]\tTest-mae:4.02632\n",
      "[403]\tTest-mae:4.02532\n",
      "[404]\tTest-mae:4.02544\n",
      "[405]\tTest-mae:4.02465\n",
      "[406]\tTest-mae:4.02422\n",
      "[407]\tTest-mae:4.02430\n",
      "[408]\tTest-mae:4.02398\n",
      "[409]\tTest-mae:4.02444\n",
      "[410]\tTest-mae:4.02369\n",
      "[411]\tTest-mae:4.02303\n",
      "[412]\tTest-mae:4.02251\n",
      "[413]\tTest-mae:4.02197\n",
      "[414]\tTest-mae:4.02087\n",
      "[415]\tTest-mae:4.02016\n",
      "[416]\tTest-mae:4.01992\n",
      "[417]\tTest-mae:4.01923\n",
      "[418]\tTest-mae:4.01802\n",
      "[419]\tTest-mae:4.01772\n",
      "[420]\tTest-mae:4.01777\n",
      "[421]\tTest-mae:4.01679\n",
      "[422]\tTest-mae:4.01709\n",
      "[423]\tTest-mae:4.01776\n",
      "[424]\tTest-mae:4.01749\n",
      "[425]\tTest-mae:4.01709\n",
      "[426]\tTest-mae:4.01663\n",
      "[427]\tTest-mae:4.01559\n",
      "[428]\tTest-mae:4.01480\n",
      "[429]\tTest-mae:4.01343\n",
      "[430]\tTest-mae:4.01439\n",
      "[431]\tTest-mae:4.01421\n",
      "[432]\tTest-mae:4.01362\n",
      "[433]\tTest-mae:4.01375\n",
      "[434]\tTest-mae:4.01362\n",
      "[435]\tTest-mae:4.01280\n",
      "[436]\tTest-mae:4.01139\n",
      "[437]\tTest-mae:4.01146\n",
      "[438]\tTest-mae:4.01072\n",
      "[439]\tTest-mae:4.01045\n",
      "[440]\tTest-mae:4.00978\n",
      "[441]\tTest-mae:4.00944\n",
      "[442]\tTest-mae:4.00903\n",
      "[443]\tTest-mae:4.00877\n",
      "[444]\tTest-mae:4.00932\n",
      "[445]\tTest-mae:4.00872\n",
      "[446]\tTest-mae:4.00900\n",
      "[447]\tTest-mae:4.00901\n",
      "[448]\tTest-mae:4.00964\n",
      "[449]\tTest-mae:4.01022\n",
      "[450]\tTest-mae:4.00897\n",
      "[451]\tTest-mae:4.00843\n",
      "[452]\tTest-mae:4.00796\n",
      "[453]\tTest-mae:4.00737\n",
      "[454]\tTest-mae:4.00614\n",
      "[455]\tTest-mae:4.00507\n",
      "[456]\tTest-mae:4.00427\n",
      "[457]\tTest-mae:4.00363\n",
      "[458]\tTest-mae:4.00358\n",
      "[459]\tTest-mae:4.00401\n",
      "[460]\tTest-mae:4.00373\n",
      "[461]\tTest-mae:4.00324\n",
      "[462]\tTest-mae:4.00402\n",
      "[463]\tTest-mae:4.00336\n",
      "[464]\tTest-mae:4.00367\n",
      "[465]\tTest-mae:4.00310\n",
      "[466]\tTest-mae:4.00498\n",
      "[467]\tTest-mae:4.00462\n",
      "[468]\tTest-mae:4.00453\n",
      "[469]\tTest-mae:4.00417\n",
      "[470]\tTest-mae:4.00462\n",
      "[471]\tTest-mae:4.00494\n",
      "[472]\tTest-mae:4.00410\n",
      "[473]\tTest-mae:4.00313\n",
      "[474]\tTest-mae:4.00267\n",
      "[475]\tTest-mae:4.00280\n",
      "[476]\tTest-mae:4.00298\n",
      "[477]\tTest-mae:4.00296\n",
      "[478]\tTest-mae:4.00247\n",
      "[479]\tTest-mae:4.00194\n",
      "[480]\tTest-mae:4.00126\n",
      "[481]\tTest-mae:4.00161\n",
      "[482]\tTest-mae:4.00193\n",
      "[483]\tTest-mae:4.00230\n",
      "[484]\tTest-mae:4.00190\n",
      "[485]\tTest-mae:4.00340\n",
      "[486]\tTest-mae:4.00353\n",
      "[487]\tTest-mae:4.00358\n",
      "[488]\tTest-mae:4.00307\n",
      "[489]\tTest-mae:4.00263\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: 4.00 in 481 rounds\n"
     ]
    }
   ],
   "source": [
    "print(\"Best MAE: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected it took us more rounds to get there, but we improved our MAE from 4.31 to 4.00. Is that good? Well it depends what you compare it to. Noting that we got this improvement almost for free, without adding data or engineering features, simply by spending a bit of time tuning our model, then it’s not bad. But it’s good to notice that it did not transform a poor model (we are still off by 4 comments on average whilst our average number of comments is 7…) into an excellent one. This is quite common with Machine Learning, whilst it is important to “roughly” tune your model to get good results from it, it will only get you that far. And there is a point after which additional time spent tuning it only provides marginal improvements. When it’s the case, it’s usually worth looking more closely at the data to find better ways of extracting information, and/or try other algorithms instead of fine tuning your current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving your model\n",
    "\n",
    "Although we found the best number of rounds, our model has been trained with more rounds than optimal, thus before using it for predictions, we should retrain it with the good number of rounds. Since we now the exact best num_boost_round, we don't need the early_stopping_round anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-mae:11.20560\n",
      "[1]\tTest-mae:11.10354\n",
      "[2]\tTest-mae:11.00289\n",
      "[3]\tTest-mae:10.90358\n",
      "[4]\tTest-mae:10.80749\n",
      "[5]\tTest-mae:10.70781\n",
      "[6]\tTest-mae:10.60834\n",
      "[7]\tTest-mae:10.51156\n",
      "[8]\tTest-mae:10.41670\n",
      "[9]\tTest-mae:10.32266\n",
      "[10]\tTest-mae:10.23176\n",
      "[11]\tTest-mae:10.14105\n",
      "[12]\tTest-mae:10.05041\n",
      "[13]\tTest-mae:9.96023\n",
      "[14]\tTest-mae:9.87409\n",
      "[15]\tTest-mae:9.78696\n",
      "[16]\tTest-mae:9.70213\n",
      "[17]\tTest-mae:9.62004\n",
      "[18]\tTest-mae:9.53587\n",
      "[19]\tTest-mae:9.45630\n",
      "[20]\tTest-mae:9.37353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:30] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\tTest-mae:9.29651\n",
      "[22]\tTest-mae:9.21901\n",
      "[23]\tTest-mae:9.14052\n",
      "[24]\tTest-mae:9.06745\n",
      "[25]\tTest-mae:8.99583\n",
      "[26]\tTest-mae:8.92462\n",
      "[27]\tTest-mae:8.85509\n",
      "[28]\tTest-mae:8.78713\n",
      "[29]\tTest-mae:8.72017\n",
      "[30]\tTest-mae:8.65116\n",
      "[31]\tTest-mae:8.58471\n",
      "[32]\tTest-mae:8.51927\n",
      "[33]\tTest-mae:8.45378\n",
      "[34]\tTest-mae:8.38722\n",
      "[35]\tTest-mae:8.32398\n",
      "[36]\tTest-mae:8.25836\n",
      "[37]\tTest-mae:8.19993\n",
      "[38]\tTest-mae:8.14226\n",
      "[39]\tTest-mae:8.08359\n",
      "[40]\tTest-mae:8.02478\n",
      "[41]\tTest-mae:7.96481\n",
      "[42]\tTest-mae:7.90802\n",
      "[43]\tTest-mae:7.85148\n",
      "[44]\tTest-mae:7.79544\n",
      "[45]\tTest-mae:7.73972\n",
      "[46]\tTest-mae:7.68544\n",
      "[47]\tTest-mae:7.63283\n",
      "[48]\tTest-mae:7.58040\n",
      "[49]\tTest-mae:7.52836\n",
      "[50]\tTest-mae:7.47415\n",
      "[51]\tTest-mae:7.42512\n",
      "[52]\tTest-mae:7.37515\n",
      "[53]\tTest-mae:7.32517\n",
      "[54]\tTest-mae:7.27579\n",
      "[55]\tTest-mae:7.22705\n",
      "[56]\tTest-mae:7.17807\n",
      "[57]\tTest-mae:7.13342\n",
      "[58]\tTest-mae:7.08749\n",
      "[59]\tTest-mae:7.04221\n",
      "[60]\tTest-mae:6.99550\n",
      "[61]\tTest-mae:6.94923\n",
      "[62]\tTest-mae:6.90776\n",
      "[63]\tTest-mae:6.86616\n",
      "[64]\tTest-mae:6.82330\n",
      "[65]\tTest-mae:6.78270\n",
      "[66]\tTest-mae:6.73928\n",
      "[67]\tTest-mae:6.69883\n",
      "[68]\tTest-mae:6.65647\n",
      "[69]\tTest-mae:6.61815\n",
      "[70]\tTest-mae:6.58137\n",
      "[71]\tTest-mae:6.54556\n",
      "[72]\tTest-mae:6.50974\n",
      "[73]\tTest-mae:6.47243\n",
      "[74]\tTest-mae:6.43474\n",
      "[75]\tTest-mae:6.39817\n",
      "[76]\tTest-mae:6.36201\n",
      "[77]\tTest-mae:6.32813\n",
      "[78]\tTest-mae:6.29388\n",
      "[79]\tTest-mae:6.26138\n",
      "[80]\tTest-mae:6.22771\n",
      "[81]\tTest-mae:6.19700\n",
      "[82]\tTest-mae:6.16595\n",
      "[83]\tTest-mae:6.13389\n",
      "[84]\tTest-mae:6.10070\n",
      "[85]\tTest-mae:6.07047\n",
      "[86]\tTest-mae:6.04090\n",
      "[87]\tTest-mae:6.00873\n",
      "[88]\tTest-mae:5.97738\n",
      "[89]\tTest-mae:5.94771\n",
      "[90]\tTest-mae:5.91897\n",
      "[91]\tTest-mae:5.88954\n",
      "[92]\tTest-mae:5.86048\n",
      "[93]\tTest-mae:5.83223\n",
      "[94]\tTest-mae:5.80675\n",
      "[95]\tTest-mae:5.77850\n",
      "[96]\tTest-mae:5.75106\n",
      "[97]\tTest-mae:5.72555\n",
      "[98]\tTest-mae:5.69836\n",
      "[99]\tTest-mae:5.67205\n",
      "[100]\tTest-mae:5.64620\n",
      "[101]\tTest-mae:5.62118\n",
      "[102]\tTest-mae:5.59898\n",
      "[103]\tTest-mae:5.57352\n",
      "[104]\tTest-mae:5.54785\n",
      "[105]\tTest-mae:5.52230\n",
      "[106]\tTest-mae:5.49721\n",
      "[107]\tTest-mae:5.47288\n",
      "[108]\tTest-mae:5.45012\n",
      "[109]\tTest-mae:5.42879\n",
      "[110]\tTest-mae:5.40863\n",
      "[111]\tTest-mae:5.38680\n",
      "[112]\tTest-mae:5.36564\n",
      "[113]\tTest-mae:5.34453\n",
      "[114]\tTest-mae:5.32340\n",
      "[115]\tTest-mae:5.30235\n",
      "[116]\tTest-mae:5.28209\n",
      "[117]\tTest-mae:5.26061\n",
      "[118]\tTest-mae:5.24083\n",
      "[119]\tTest-mae:5.22314\n",
      "[120]\tTest-mae:5.20296\n",
      "[121]\tTest-mae:5.18561\n",
      "[122]\tTest-mae:5.16384\n",
      "[123]\tTest-mae:5.14687\n",
      "[124]\tTest-mae:5.12900\n",
      "[125]\tTest-mae:5.11118\n",
      "[126]\tTest-mae:5.09245\n",
      "[127]\tTest-mae:5.07884\n",
      "[128]\tTest-mae:5.06039\n",
      "[129]\tTest-mae:5.04584\n",
      "[130]\tTest-mae:5.03120\n",
      "[131]\tTest-mae:5.01339\n",
      "[132]\tTest-mae:4.99617\n",
      "[133]\tTest-mae:4.98127\n",
      "[134]\tTest-mae:4.96698\n",
      "[135]\tTest-mae:4.95431\n",
      "[136]\tTest-mae:4.94109\n",
      "[137]\tTest-mae:4.92940\n",
      "[138]\tTest-mae:4.91580\n",
      "[139]\tTest-mae:4.90446\n",
      "[140]\tTest-mae:4.89030\n",
      "[141]\tTest-mae:4.87900\n",
      "[142]\tTest-mae:4.86567\n",
      "[143]\tTest-mae:4.85541\n",
      "[144]\tTest-mae:4.84464\n",
      "[145]\tTest-mae:4.83285\n",
      "[146]\tTest-mae:4.81967\n",
      "[147]\tTest-mae:4.80809\n",
      "[148]\tTest-mae:4.79548\n",
      "[149]\tTest-mae:4.78401\n",
      "[150]\tTest-mae:4.77270\n",
      "[151]\tTest-mae:4.76278\n",
      "[152]\tTest-mae:4.75288\n",
      "[153]\tTest-mae:4.74191\n",
      "[154]\tTest-mae:4.73426\n",
      "[155]\tTest-mae:4.72505\n",
      "[156]\tTest-mae:4.71488\n",
      "[157]\tTest-mae:4.70714\n",
      "[158]\tTest-mae:4.69726\n",
      "[159]\tTest-mae:4.68777\n",
      "[160]\tTest-mae:4.67886\n",
      "[161]\tTest-mae:4.66854\n",
      "[162]\tTest-mae:4.65988\n",
      "[163]\tTest-mae:4.65092\n",
      "[164]\tTest-mae:4.64035\n",
      "[165]\tTest-mae:4.62994\n",
      "[166]\tTest-mae:4.62232\n",
      "[167]\tTest-mae:4.61161\n",
      "[168]\tTest-mae:4.60422\n",
      "[169]\tTest-mae:4.59444\n",
      "[170]\tTest-mae:4.58616\n",
      "[171]\tTest-mae:4.57824\n",
      "[172]\tTest-mae:4.56940\n",
      "[173]\tTest-mae:4.56067\n",
      "[174]\tTest-mae:4.55371\n",
      "[175]\tTest-mae:4.54590\n",
      "[176]\tTest-mae:4.53760\n",
      "[177]\tTest-mae:4.53040\n",
      "[178]\tTest-mae:4.52195\n",
      "[179]\tTest-mae:4.51491\n",
      "[180]\tTest-mae:4.50829\n",
      "[181]\tTest-mae:4.50157\n",
      "[182]\tTest-mae:4.49299\n",
      "[183]\tTest-mae:4.48468\n",
      "[184]\tTest-mae:4.47711\n",
      "[185]\tTest-mae:4.47109\n",
      "[186]\tTest-mae:4.46426\n",
      "[187]\tTest-mae:4.45906\n",
      "[188]\tTest-mae:4.45273\n",
      "[189]\tTest-mae:4.44633\n",
      "[190]\tTest-mae:4.44097\n",
      "[191]\tTest-mae:4.43687\n",
      "[192]\tTest-mae:4.43264\n",
      "[193]\tTest-mae:4.42822\n",
      "[194]\tTest-mae:4.42128\n",
      "[195]\tTest-mae:4.41572\n",
      "[196]\tTest-mae:4.41065\n",
      "[197]\tTest-mae:4.40655\n",
      "[198]\tTest-mae:4.40207\n",
      "[199]\tTest-mae:4.39722\n",
      "[200]\tTest-mae:4.39152\n",
      "[201]\tTest-mae:4.38554\n",
      "[202]\tTest-mae:4.38088\n",
      "[203]\tTest-mae:4.37566\n",
      "[204]\tTest-mae:4.37048\n",
      "[205]\tTest-mae:4.36722\n",
      "[206]\tTest-mae:4.36360\n",
      "[207]\tTest-mae:4.35864\n",
      "[208]\tTest-mae:4.35402\n",
      "[209]\tTest-mae:4.34781\n",
      "[210]\tTest-mae:4.34222\n",
      "[211]\tTest-mae:4.33666\n",
      "[212]\tTest-mae:4.33353\n",
      "[213]\tTest-mae:4.32862\n",
      "[214]\tTest-mae:4.32405\n",
      "[215]\tTest-mae:4.31946\n",
      "[216]\tTest-mae:4.31468\n",
      "[217]\tTest-mae:4.30835\n",
      "[218]\tTest-mae:4.30346\n",
      "[219]\tTest-mae:4.29827\n",
      "[220]\tTest-mae:4.29403\n",
      "[221]\tTest-mae:4.28947\n",
      "[222]\tTest-mae:4.28619\n",
      "[223]\tTest-mae:4.28134\n",
      "[224]\tTest-mae:4.27793\n",
      "[225]\tTest-mae:4.27246\n",
      "[226]\tTest-mae:4.27005\n",
      "[227]\tTest-mae:4.26564\n",
      "[228]\tTest-mae:4.26104\n",
      "[229]\tTest-mae:4.25876\n",
      "[230]\tTest-mae:4.25650\n",
      "[231]\tTest-mae:4.25401\n",
      "[232]\tTest-mae:4.25195\n",
      "[233]\tTest-mae:4.24888\n",
      "[234]\tTest-mae:4.24630\n",
      "[235]\tTest-mae:4.24291\n",
      "[236]\tTest-mae:4.23889\n",
      "[237]\tTest-mae:4.23569\n",
      "[238]\tTest-mae:4.23348\n",
      "[239]\tTest-mae:4.23026\n",
      "[240]\tTest-mae:4.22786\n",
      "[241]\tTest-mae:4.22409\n",
      "[242]\tTest-mae:4.22143\n",
      "[243]\tTest-mae:4.21938\n",
      "[244]\tTest-mae:4.21616\n",
      "[245]\tTest-mae:4.21302\n",
      "[246]\tTest-mae:4.20939\n",
      "[247]\tTest-mae:4.20659\n",
      "[248]\tTest-mae:4.20284\n",
      "[249]\tTest-mae:4.20088\n",
      "[250]\tTest-mae:4.19750\n",
      "[251]\tTest-mae:4.19642\n",
      "[252]\tTest-mae:4.19382\n",
      "[253]\tTest-mae:4.19188\n",
      "[254]\tTest-mae:4.19042\n",
      "[255]\tTest-mae:4.18795\n",
      "[256]\tTest-mae:4.18540\n",
      "[257]\tTest-mae:4.18366\n",
      "[258]\tTest-mae:4.18244\n",
      "[259]\tTest-mae:4.18036\n",
      "[260]\tTest-mae:4.17927\n",
      "[261]\tTest-mae:4.17673\n",
      "[262]\tTest-mae:4.17461\n",
      "[263]\tTest-mae:4.17403\n",
      "[264]\tTest-mae:4.17153\n",
      "[265]\tTest-mae:4.16979\n",
      "[266]\tTest-mae:4.16866\n",
      "[267]\tTest-mae:4.16757\n",
      "[268]\tTest-mae:4.16535\n",
      "[269]\tTest-mae:4.16318\n",
      "[270]\tTest-mae:4.16127\n",
      "[271]\tTest-mae:4.15849\n",
      "[272]\tTest-mae:4.15700\n",
      "[273]\tTest-mae:4.15611\n",
      "[274]\tTest-mae:4.15417\n",
      "[275]\tTest-mae:4.15278\n",
      "[276]\tTest-mae:4.15108\n",
      "[277]\tTest-mae:4.14909\n",
      "[278]\tTest-mae:4.14736\n",
      "[279]\tTest-mae:4.14692\n",
      "[280]\tTest-mae:4.14584\n",
      "[281]\tTest-mae:4.14468\n",
      "[282]\tTest-mae:4.14203\n",
      "[283]\tTest-mae:4.14064\n",
      "[284]\tTest-mae:4.13957\n",
      "[285]\tTest-mae:4.13929\n",
      "[286]\tTest-mae:4.13617\n",
      "[287]\tTest-mae:4.13565\n",
      "[288]\tTest-mae:4.13310\n",
      "[289]\tTest-mae:4.13109\n",
      "[290]\tTest-mae:4.12905\n",
      "[291]\tTest-mae:4.12562\n",
      "[292]\tTest-mae:4.12374\n",
      "[293]\tTest-mae:4.12170\n",
      "[294]\tTest-mae:4.12147\n",
      "[295]\tTest-mae:4.11974\n",
      "[296]\tTest-mae:4.11878\n",
      "[297]\tTest-mae:4.11724\n",
      "[298]\tTest-mae:4.11500\n",
      "[299]\tTest-mae:4.11383\n",
      "[300]\tTest-mae:4.11106\n",
      "[301]\tTest-mae:4.10970\n",
      "[302]\tTest-mae:4.10927\n",
      "[303]\tTest-mae:4.10698\n",
      "[304]\tTest-mae:4.10535\n",
      "[305]\tTest-mae:4.10463\n",
      "[306]\tTest-mae:4.10389\n",
      "[307]\tTest-mae:4.10406\n",
      "[308]\tTest-mae:4.10290\n",
      "[309]\tTest-mae:4.10153\n",
      "[310]\tTest-mae:4.10084\n",
      "[311]\tTest-mae:4.10054\n",
      "[312]\tTest-mae:4.10015\n",
      "[313]\tTest-mae:4.09877\n",
      "[314]\tTest-mae:4.09594\n",
      "[315]\tTest-mae:4.09462\n",
      "[316]\tTest-mae:4.09254\n",
      "[317]\tTest-mae:4.09154\n",
      "[318]\tTest-mae:4.09173\n",
      "[319]\tTest-mae:4.09077\n",
      "[320]\tTest-mae:4.09074\n",
      "[321]\tTest-mae:4.09047\n",
      "[322]\tTest-mae:4.08936\n",
      "[323]\tTest-mae:4.08879\n",
      "[324]\tTest-mae:4.08878\n",
      "[325]\tTest-mae:4.08831\n",
      "[326]\tTest-mae:4.08654\n",
      "[327]\tTest-mae:4.08555\n",
      "[328]\tTest-mae:4.08452\n",
      "[329]\tTest-mae:4.08314\n",
      "[330]\tTest-mae:4.08204\n",
      "[331]\tTest-mae:4.08059\n",
      "[332]\tTest-mae:4.07939\n",
      "[333]\tTest-mae:4.07897\n",
      "[334]\tTest-mae:4.07779\n",
      "[335]\tTest-mae:4.07662\n",
      "[336]\tTest-mae:4.07397\n",
      "[337]\tTest-mae:4.07306\n",
      "[338]\tTest-mae:4.07178\n",
      "[339]\tTest-mae:4.07044\n",
      "[340]\tTest-mae:4.06975\n",
      "[341]\tTest-mae:4.06899\n",
      "[342]\tTest-mae:4.06806\n",
      "[343]\tTest-mae:4.06554\n",
      "[344]\tTest-mae:4.06514\n",
      "[345]\tTest-mae:4.06383\n",
      "[346]\tTest-mae:4.06235\n",
      "[347]\tTest-mae:4.06154\n",
      "[348]\tTest-mae:4.05998\n",
      "[349]\tTest-mae:4.05854\n",
      "[350]\tTest-mae:4.05681\n",
      "[351]\tTest-mae:4.05626\n",
      "[352]\tTest-mae:4.05554\n",
      "[353]\tTest-mae:4.05601\n",
      "[354]\tTest-mae:4.05522\n",
      "[355]\tTest-mae:4.05447\n",
      "[356]\tTest-mae:4.05369\n",
      "[357]\tTest-mae:4.05218\n",
      "[358]\tTest-mae:4.05155\n",
      "[359]\tTest-mae:4.05079\n",
      "[360]\tTest-mae:4.05010\n",
      "[361]\tTest-mae:4.04901\n",
      "[362]\tTest-mae:4.04892\n",
      "[363]\tTest-mae:4.04862\n",
      "[364]\tTest-mae:4.04812\n",
      "[365]\tTest-mae:4.04819\n",
      "[366]\tTest-mae:4.04775\n",
      "[367]\tTest-mae:4.04630\n",
      "[368]\tTest-mae:4.04438\n",
      "[369]\tTest-mae:4.04388\n",
      "[370]\tTest-mae:4.04395\n",
      "[371]\tTest-mae:4.04302\n",
      "[372]\tTest-mae:4.04216\n",
      "[373]\tTest-mae:4.04190\n",
      "[374]\tTest-mae:4.04135\n",
      "[375]\tTest-mae:4.03983\n",
      "[376]\tTest-mae:4.03963\n",
      "[377]\tTest-mae:4.03916\n",
      "[378]\tTest-mae:4.04069\n",
      "[379]\tTest-mae:4.03949\n",
      "[380]\tTest-mae:4.03859\n",
      "[381]\tTest-mae:4.03775\n",
      "[382]\tTest-mae:4.03688\n",
      "[383]\tTest-mae:4.03567\n",
      "[384]\tTest-mae:4.03475\n",
      "[385]\tTest-mae:4.03409\n",
      "[386]\tTest-mae:4.03237\n",
      "[387]\tTest-mae:4.03162\n",
      "[388]\tTest-mae:4.03116\n",
      "[389]\tTest-mae:4.03054\n",
      "[390]\tTest-mae:4.03023\n",
      "[391]\tTest-mae:4.02995\n",
      "[392]\tTest-mae:4.03035\n",
      "[393]\tTest-mae:4.02918\n",
      "[394]\tTest-mae:4.02839\n",
      "[395]\tTest-mae:4.02822\n",
      "[396]\tTest-mae:4.02827\n",
      "[397]\tTest-mae:4.02810\n",
      "[398]\tTest-mae:4.02773\n",
      "[399]\tTest-mae:4.02769\n",
      "[400]\tTest-mae:4.02669\n",
      "[401]\tTest-mae:4.02664\n",
      "[402]\tTest-mae:4.02632\n",
      "[403]\tTest-mae:4.02532\n",
      "[404]\tTest-mae:4.02544\n",
      "[405]\tTest-mae:4.02465\n",
      "[406]\tTest-mae:4.02422\n",
      "[407]\tTest-mae:4.02430\n",
      "[408]\tTest-mae:4.02398\n",
      "[409]\tTest-mae:4.02444\n",
      "[410]\tTest-mae:4.02369\n",
      "[411]\tTest-mae:4.02303\n",
      "[412]\tTest-mae:4.02251\n",
      "[413]\tTest-mae:4.02197\n",
      "[414]\tTest-mae:4.02087\n",
      "[415]\tTest-mae:4.02016\n",
      "[416]\tTest-mae:4.01992\n",
      "[417]\tTest-mae:4.01923\n",
      "[418]\tTest-mae:4.01802\n",
      "[419]\tTest-mae:4.01772\n",
      "[420]\tTest-mae:4.01777\n",
      "[421]\tTest-mae:4.01679\n",
      "[422]\tTest-mae:4.01709\n",
      "[423]\tTest-mae:4.01776\n",
      "[424]\tTest-mae:4.01749\n",
      "[425]\tTest-mae:4.01709\n",
      "[426]\tTest-mae:4.01663\n",
      "[427]\tTest-mae:4.01559\n",
      "[428]\tTest-mae:4.01480\n",
      "[429]\tTest-mae:4.01343\n",
      "[430]\tTest-mae:4.01439\n",
      "[431]\tTest-mae:4.01421\n",
      "[432]\tTest-mae:4.01362\n",
      "[433]\tTest-mae:4.01375\n",
      "[434]\tTest-mae:4.01362\n",
      "[435]\tTest-mae:4.01280\n",
      "[436]\tTest-mae:4.01139\n",
      "[437]\tTest-mae:4.01146\n",
      "[438]\tTest-mae:4.01072\n",
      "[439]\tTest-mae:4.01045\n",
      "[440]\tTest-mae:4.00978\n",
      "[441]\tTest-mae:4.00944\n",
      "[442]\tTest-mae:4.00903\n",
      "[443]\tTest-mae:4.00877\n",
      "[444]\tTest-mae:4.00932\n",
      "[445]\tTest-mae:4.00872\n",
      "[446]\tTest-mae:4.00900\n",
      "[447]\tTest-mae:4.00901\n",
      "[448]\tTest-mae:4.00964\n",
      "[449]\tTest-mae:4.01022\n",
      "[450]\tTest-mae:4.00897\n",
      "[451]\tTest-mae:4.00843\n",
      "[452]\tTest-mae:4.00796\n",
      "[453]\tTest-mae:4.00737\n",
      "[454]\tTest-mae:4.00614\n",
      "[455]\tTest-mae:4.00507\n",
      "[456]\tTest-mae:4.00427\n",
      "[457]\tTest-mae:4.00363\n",
      "[458]\tTest-mae:4.00358\n",
      "[459]\tTest-mae:4.00401\n",
      "[460]\tTest-mae:4.00373\n",
      "[461]\tTest-mae:4.00324\n",
      "[462]\tTest-mae:4.00402\n",
      "[463]\tTest-mae:4.00336\n",
      "[464]\tTest-mae:4.00367\n",
      "[465]\tTest-mae:4.00310\n",
      "[466]\tTest-mae:4.00498\n",
      "[467]\tTest-mae:4.00462\n",
      "[468]\tTest-mae:4.00453\n",
      "[469]\tTest-mae:4.00417\n",
      "[470]\tTest-mae:4.00462\n",
      "[471]\tTest-mae:4.00494\n",
      "[472]\tTest-mae:4.00410\n",
      "[473]\tTest-mae:4.00313\n",
      "[474]\tTest-mae:4.00267\n",
      "[475]\tTest-mae:4.00280\n",
      "[476]\tTest-mae:4.00298\n",
      "[477]\tTest-mae:4.00296\n",
      "[478]\tTest-mae:4.00247\n",
      "[479]\tTest-mae:4.00194\n",
      "[480]\tTest-mae:4.00126\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = model.best_iteration + 1\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good, now let’s use our model to make predictions. We will use the test dataset and compute MAE with the scikit-learn function. We should obtain the same score as promised in the last round of training, let’s check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.001262541873988"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(best_model.predict(dtest), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! If you want to re-use your model on new data in the future, it can be a good idea to save it to a file, here is how you can do it with XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:32] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "best_model.save_model(\"my_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load the model later with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:160: UserWarning: [21:59:32] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_b6b50u1t0e/croot/xgboost-split_1713972723244/work/cpp_src/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.487469  , 0.37458023, 2.1316977 , ..., 4.379968  , 0.11489217,\n",
       "       4.28442   ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = xgb.Booster()\n",
    "loaded_model.load_model(\"my_model.model\")\n",
    "\n",
    "# And use it for predictions.\n",
    "loaded_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "\n",
    "- https://archive.ics.uci.edu/dataset/363/facebook+comment+volume+dataset\n",
    "- https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
