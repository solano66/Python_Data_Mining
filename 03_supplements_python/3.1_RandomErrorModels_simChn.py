'''Collated by Prof. Ching-Shih Tsou 邹庆士 教授 (Ph.D.) at the IDS, CICD, and CISD of NTUB (台北商业大学资讯与决策科学研究所暨智能控制与决策研究室教授兼校务永续发展中心主任); at the ME Dept. and CAIDS of MCUT (2020年借调至明志科技大学机械工程系担任特聘教授兼人工智慧暨资料科学研究中心主任两年); the CSQ (2019年起任品质学会大数据品质应用委员会主任委员); the DSBA (2013年创立台湾资料科学与商业应用协会); and the CARS (2012年创立中华R软体学会)Dataset: algae.csvNotes: This code is provided without warranty.'''#### 3.1 随机误差模型 How to find a robust random error or probabilitic model ?import pandas as pdimport numpy as np# pd.set_option('display.max_rows', 500)# pd.set_option('display.max_columns', 500)algae = pd.read_csv('algae.csv', encoding='utf-8')# 资料分析工作中，经常需要将文字资料转为数值(又称编码encoding or coding)，以利后续的数学建模，常用的编码方式有标签编码(label encoding)、单热编码(one-hot encoding)、以及统计学中的虚拟编码(dummy encoding)。R语言读入资料集后可以自动进行标签编码，亦即将字串自动转换为因子，而Python常须手动编码字串变量，其中套件scikit-learn偏好单热编码，R语言则偏好标签及虚拟编码。 We have to map the categorical variables to integers.algae['season'] = algae['season'].map({'spring':1,'summer':2,'autumn':3,'winter':4}).astype(int) # label encoding(Python dict其实就是一个对应关系)algae['size'] = algae['size'].map({'small':1,'medium':2,'large':3}).astype(int)algae['speed'] = algae['speed'].map({'low':1,'medium':2,'high':3}).astype(int)# 各变量遗缺状况统计表algae.isnull().sum() # sum(is.na(algae)) in R, 是把二维表中所有真假值加总起来(rowSums()与columnSums())# 各样本遗缺状况统计表algae.isnull().sum(axis=1)# 遗缺值超过4个变量的样本编号algae.isnull().sum(axis='columns')[algae.isnull().sum(axis='columns') > 4]# 移除遗缺程度严重的样本cleanAlgae_tmp = algae.dropna(axis='rows',thresh=13) # 变数个数大于等于13者留之# 以各变项中位数填补遗缺值 Imputations on missing datacleanAlgae = pd.DataFrame()for col in cleanAlgae_tmp.columns:    cleanAlgae[col] = cleanAlgae_tmp[col].fillna(cleanAlgae_tmp[col].median())type(cleanAlgae_tmp.columns)# algae['mxPH'].median()# 确认资料表中已无遗缺值cleanAlgae.isnull().sum()# 绘制散布图，探索变量关系(不置入书中)targets = ['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7']predictors = ['season','size','speed','mxPH','mnO2','Cl','NO3','NH4','oPO4','PO4','Chla']import matplotlib.pyplot as pltimport seaborn as snsfor feature in cleanAlgae.columns[:11]:    p = sns.scatterplot(data=cleanAlgae.loc[:, predictors+['a1']], x=feature, y='a1', palette='RdBu')    p.set(xscale='log', yscale='log')    plt.show()# 选择X变数以及y变数 Select the features and targetX = cleanAlgae[['season','size','speed','mxPH','mnO2','Cl','NO3','NH4','oPO4','PO4','Chla']]y = cleanAlgae[['a1']]# 切割训练集与测试集(乱数种子1234) Use the simple train-test split (under random seed 1234)from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=50, random_state=1234)# 以148个训练样本估计函数关系#### 线性回归法一：使用scikit-learn.linear_model的LinearRegression类别 (by scipy.linalg.lstsq 未提供统计检定报表)# 注：如用梯度陡降法逼近回归系数，也是没有检定报表！# Step 1 载入建模所需类别函数 Import the necessary librariesfrom sklearn.linear_model import LinearRegression# Step 2 宣告空模(假设线性，但参数未知的模型)a1Lm = LinearRegression()pre = dir(a1Lm) # 空模属性与方法# Step 3 将训练样本传入，配适/拟合模型参数，空模转为实模a1Lm.fit(X_train, y_train)post = dir(a1Lm)set(post) - set(pre) # Python不成文的规定，资料传入配适模型完成后，新增的属性与方法多带下底线！请留意intercept_和coef_# 11个变数的回归系数(为何不是15个？类别变数未虚拟编码！)a1Lm.coef_ # 11个斜率系数，Why? without dummy coding (one-hot encoding) for categorical variablesa1Lm.coef_.shape# 回归模型配适完后的属性与方法# 截距系数a1Lm.intercept_# 特征矩阵X的秩a1Lm.rank_# 特征矩阵X的奇异值(https://baike.baidu.hk/item/%E5%A5%87%E7%95%B0%E5%80%BC/9975162)a1Lm.singular_# 特征矩阵X的特征个数a1Lm.n_features_in_# 特征矩阵X的特征名称a1Lm.feature_names_in_# 取得模型的参数a1Lm.get_params()# Step 4 拟合完成后运用模型a1Lm 估计训练样本的a1 有害藻类浓度trainPred = a1Lm.predict(X_train)# 训练样本的模型绩效指标RMSE值(参见3.2.1节)from sklearn import metricstrainRMSE = np.sqrt(metrics.mean_squared_error(y_train, trainPred))print("训练样本的RMSE值为 The RMSE for 148 training samples：{}".format(trainRMSE))# Step 4 以模型a1Lm估计测试样本的a1有害藻类浓度testPred = a1Lm.predict(X_test)# 测试样本的模型绩效指标RMSE值testRMSE = np.sqrt(metrics.mean_squared_error(y_test, testPred))print("测试样本的RMSE值为 A more reliable estimate of RMSE is from the test set：{}".format(testRMSE))algae[['a1']].describe()# 下面是测试集的RMSE比训练集RMSE还低的结果(乱数种子20531) I want to show you if I change the random seed from 1234 to 20531, then the metrics RMSE are upside down ! np.random.seed(20531)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)# 以重新切分后的148个训练样本估计函数关系from sklearn.linear_model import LinearRegressiona1Lm2 = LinearRegression() # a1Lm2 = LinearRegression().fit(X_train, y_train)a1Lm2.fit(X_train, y_train)dir(a1Lm2)# 11个变数的回归系数(类别变数未虚拟编码！)a1Lm2.coef_ # without dummy coding (one-hot encoding) for categorical variables# 截距系数a1Lm2.intercept_# 拟合完成后运用模型a1Lm2估计训练样本的a1有害藻类浓度trainPred = a1Lm2.predict(X_train)# 训练样本的模型绩效指标RMSE值(参见3.2.1节)from sklearn import metricstrainRMSE = np.sqrt(metrics.mean_squared_error(y_train,trainPred))print("训练样本的RMSE值为 The RMSE for 148 training samples under 20531：{}".format(trainRMSE))# 以模型a1Lm2估计测试样本的a1有害藻类浓度testPred = a1Lm2.predict(X_test)# 测试样本的模型绩效指标RMSE值testRMSE = np.sqrt(metrics.mean_squared_error(y_test,testPred))print("测试样本的RMSE值为 A more reliable estimate of RMSE is from the test set：{}，低于训练样本的RMSE值{}".format(testRMSE, trainRMSE))# 怎么会这样！？乱数种子好恐怖也！其实随机误差模型是依训练样本配适均值模型，要找到稳健性或鲁棒性高的模型，其解决之道是反覆进行多次实验(eg. **repeated** train-test split/hold-out, bootstrapping, or 10-fold cross-validation)# Different random seeds get different results ! Which one should we rely on? The answer is that we need to conduct repeated experiments again and again. And average out the performance across the experiments to understand the variation/confidence behind the probabilistic models, in order to get a robust one.#### 遗憾之处：没看到因子变量的虚拟编码、系数的t检定、模型整体配适度F检定、残差检定！#### 线性回归法二：使用统计报表较完整的statsmodels套件(Why? Because of R.)(用矩阵代数求解回归系数)# 语法一：R model formula (数学统计人群)# 为了后续使用 model formula (统计惯用的建模语法，来自R语言)cleanAlgae_train = pd.concat([X_train, y_train], axis='columns')# Step 1import statsmodels.formula.api as smf# Steps 2 & 3# ols stands for Ordinary Least Squaresa1Lm3 = smf.ols('a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla', data=cleanAlgae_train).fit()# SyntaxError: invalid syntax in the shorthand . in Rdir(a1Lm3)a1Lm3.summary() # 有一点失望～(没看到因子变量的虚拟编码)type(a1Lm3.summary()) # statsmodels.iolib.summary.Summaryresults_summary = a1Lm3.summary()# Note that tables is a list. The table at index 1 is the "core" table. Additionally, read_html puts dfs in a list, so we want index 0type(results_summary.tables)len(results_summary.tables)results_as_html = results_summary.tables[0].as_html()pd.read_html(results_as_html, header=0, index_col=0)[0]results_as_html = results_summary.tables[1].as_html()pd.read_html(results_as_html, header=0, index_col=0)[0]results_as_html = results_summary.tables[2].as_html()pd.read_html(results_as_html, header=0, index_col=0)[0]# 语法二：statsmodels (丢属性矩阵X_train及反应变数向量y_train，计算机人群)import statsmodels.api as sma1Lm4 = sm.OLS(y_train, X_train).fit()dir(a1Lm4)a1Lm4.summary() # 报表同上#### 线性回归法三：如何产生与R语言或统计书上相同的虚拟变数编码报表 Dummy coding in Python {statsmodels}# Search terms by 'statsmodel dummy coding'# https://douglaspsteen.github.io/handling_categorical_variables_with_statsmodels_olsimport pandas as pd# *资料要重新读入*algae = pd.read_csv('algae.csv', encoding='utf-8', dtype={'season': 'category', 'size': 'category', 'speed': 'category'}) # pandas的'category'类似R的'factor'algae.dtypes # 留意前三栏为category型别(原来为object)# 移除遗缺的样本cleanAlgae2 = algae.dropna(axis='rows') # (184, 18) 此处不将重度遗缺(直接删除)和轻度遗缺(用中位数填补)样本分开处理cleanAlgae2.dtypes # The first three are not objects again ! They are category.# 选择X变数以及y变数X = cleanAlgae2[['season','size','speed','mxPH','mnO2','Cl','NO3','NH4','oPO4','PO4','Chla']]y = cleanAlgae2[['a1']]from sklearn.model_selection import train_test_split# 切割训练集与测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=50, random_state=1234)import statsmodels.formula.api as smf# Please use C() to enclose your category variables. *模型公式符号中请用C()包住类别变数(虚拟编码)*f_rev = 'a1 ~ C(season) + C(size) + C(speed) + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla'cleanAlgae2_train = pd.concat([X_train, y_train], axis=1)model_rev = smf.ols(formula=f_rev, data=cleanAlgae2_train).fit()model_rev.summary()# 拟合完成后运用模型a1Lm2估计训练样本的a1有害藻类浓度trainPred = model_rev.predict(X_train)# 训练样本的模型绩效指标RMSE值(参见3.2.1节)from sklearn import metricstrainRMSE = np.sqrt(metrics.mean_squared_error(y_train,trainPred))print("训练样本的RMSE值为：{}".format(trainRMSE))# 以模型a1Lm2估计测试样本的a1有害藻类浓度testPred = model_rev.predict(X_test)# 测试样本的模型绩效指标RMSE值testRMSE = np.sqrt(metrics.mean_squared_error(y_test,testPred))print("测试样本的RMSE值为：{}，高于训练样本的RMSE值{}".format(testRMSE, trainRMSE))#### Difference between statsmodel OLS (为了解释，细节较多) and scikit linear regression (为了准确预测)# https://stats.stackexchange.com/questions/146804/difference-between-statsmodel-ols-and-scikit-linear-regression# Statsmodels follows largely the traditional model where we want to know how well a given model fits the data, and what variables "explain" or affect the outcome, or what the size of the effect is. Scikit-learn follows the machine learning tradition where the main supported task is chosing the "best" model for prediction. 配适状况是否良好、解释或影响结果(因)变量的(自)变数有哪些、或影响的大小; 预测最佳的模型#### Interpreting Linear Regression Through statsmodels.summary()# https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a# https://www.adrian.idv.hk/2021-07-16-statsmodels/# Df Residuals is another name for our Degrees of Freedom in our mode. This is calculated in the form of ‘n-k-1’ or ‘number of observations-number of predicting variables-1.’ Df Model numbers our predicting variables. If you’re wondering why we only entered 3 predicting variables into the formula but both Df Residuals and Model are saying there are 6, we’ll get into this later. Our Covariance Type is listed as nonrobust. Covariance is a measure of how two variables are linked in a positive or negative manner, and a robust covariance is one that is calculated in a way to minimize or eliminate variables, which is not the case here.# Log-likelihood is a numerical signifier of the likelihood that your produced model produced the given data. 模型产生给定资料的可能性 It is used to compare coefficient values for each variable in the process of creating the model. AIC and BIC are both used to compare the efficacy of models in the process of linear regression, using a penalty system for measuring multiple variables. These numbers are used for feature selection of variables.# Omnibus describes the normalcy of the distribution of our residuals using skew and kurtosis as measurements. 利用偏态与峰度检验残差的常态性 A 0 would indicate perfect normalcy. Prob(Omnibus) is a statistical test measuring the probability the residuals are normally distributed. A 1 would indicate perfectly normal distribution. Skew is a measurement of symmetry in our data, with 0 being perfect symmetry. Kurtosis measures the peakiness of our data, or its concentration around 0 in a normal curve. Higher kurtosis implies fewer outliers.# Durbin-Watson is a measurement of homoscedasticity, or an even distribution of errors throughout our data. (i.e. autocorrelation in the residuals) Heteroscedasticity would imply an uneven distribution, for example as the data point grows higher the relative error grows higher. Ideal homoscedasticity will lie between 1 and 2. # Jarque-Bera (JB) and Prob(JB) are alternate methods of measuring the same value as Omnibus and Prob(Omnibus) using skewness and kurtosis. We use these values to confirm each other. # Condition number 条件数 is a measurement of the sensitivity of our model as compared to the size of changes in the data it is analyzing. 模型敏感度，亦即数据变动后模型的变动幅度(https://blog.csdn.net/lanchunhui/article/details/51372831) Multicollinearity is strongly implied by a high condition number. 高条件数显示自变量间存在多重共线性 Multicollinearity a term to describe two or more independent variables that are strongly related to each other and are falsely affecting our predicted variable by redundancy.