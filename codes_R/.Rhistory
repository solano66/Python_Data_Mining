mean(simlist)
install.packages("dplyr")
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
?loess
require(graphics)
with(cars, scatter.smooth(speed, dist))
## or with dotted thick smoothed line results :
with(cars, scatter.smooth(speed, dist, lpars =
list(col = "red", lwd = 3, lty = 3)))
?seq
?prcomp
library(AppliedPredictiveModeling)
data(segmentationOriginal)
## ----eval=FALSE----------------------------------------------------------
## str(segmentationOriginal, list.len = 10)
## ----warning=FALSE, message=FALSE----------------------------------------
library(UsingR)
# 119個變數名稱，限於篇幅，只顯示頭尾
headtail(names(segmentationOriginal))
# 目標變數`Class`次數分佈`table`(segmentationOriginal$Case)
## ----eval=FALSE----------------------------------------------------------
## # 報表太大，請讀者自行執行
## summary(segmentationOriginal)
## ------------------------------------------------------------------------
# 自變項資料框傳入sapply()，以匿名函數計算各變數最小與最大值
headtail(sapply(segmentationOriginal[-(1:3)], function(u)
c(min = min(u), max = max(u))))
## ------------------------------------------------------------------------
# 因子屬性`Case`次數分佈`table`(segmentationOriginal$Case)
segTrain <- subset(segmentationOriginal, Case == "Train")
# table(segmentationOriginal$Case)
## ------------------------------------------------------------------------
# 類別標籤向量獨立為segTrainClass，1009個類別標籤值
segTrainClass <- segTrain$Class
length(segTrainClass)
## ------------------------------------------------------------------------
segTrainX <- segTrain[, -(1:3)]
# 1009個訓練樣本，116個屬性
dim(segTrainX)
## ----warning=FALSE, message=FALSE----------------------------------------
# 分類與迴歸訓練重要R套件
library(caret)
# 預設傳回近乎零變異的變數編號
nearZeroVar(segTrainX)
# ?nearZeroVar
# 近乎零變異變數名稱
names(segTrainX)[nearZeroVar(segTrainX)]
# 移除近乎零變異變數後存為segTrainXV
segTrainXV <- segTrainX[, -nearZeroVar(segTrainX)]
#table(segTrainX$MemberAvgAvgIntenStatusCh2)
## ------------------------------------------------------------------------
# 改變引數(saveMetrics)設定值，取得完整報表(報表很長，只檢視某些變數結果)
nearZeroVar(segTrainX, saveMetrics = TRUE)[68:75,]
# 輸出報表類別為資料框
class(nearZeroVar(segTrainX, saveMetrics = TRUE))
str(nearZeroVar(segTrainX, saveMetrics = TRUE))
# 零變異變數名稱
names(segTrainX)[nearZeroVar(segTrainX, saveMetrics =
TRUE)$zeroVar]
## ------------------------------------------------------------------------
# 擷取名稱帶有"Status"的變數編號
(statusColNum <- grep("Status", names(segTrainXV)))
## ------------------------------------------------------------------------
# 名稱有"Status"之變數成批產生次數分配表，確定均為三元以下變數
head(sapply(segTrainXV[, statusColNum], table))
# 分離出數值屬性矩陣segTrainXNC，NC表not categorical
segTrainXNC <- segTrainXV[, -statusColNum]
## ------------------------------------------------------------------------
# VarIntenCh3嚴重右偏
summary(segTrainXNC$VarIntenCh3)
max(segTrainX$VarIntenCh3)/min(segTrainX$VarIntenCh3)
## ----warning=FALSE, message=FALSE----------------------------------------
# 偏態係數計算
library(e1071)
skewness(segTrainXNC$VarIntenCh3)
## ----fig.align="center", fig.cap = "\\label{fig:VarIntenCh3_histden}肌動蛋白絲畫素強度標準差之直方圖與密度曲線"----
# 視覺化檢驗偏態狀況
hist(segTrainXNC$VarIntenCh3, prob = TRUE, ylim =
c(0, 0.009), xlab = 'VarIntenCh3')
lines(density(segTrainXNC$VarIntenCh3))
# 以位置量數細部查驗偏態狀況(注意66%與97%)
quantile(segTrainXNC$VarIntenCh3, probs = seq(0, 1, 0.01))
## ------------------------------------------------------------------------
# 以apply()+sort()計算並排序所有數值變數的偏態係數
skewValues <- apply(segTrainXNC, 2, skewness)
headtail(sort(skewValues, decreasing = TRUE))
## ----fig.align="center", fig.cap = "\\label{fig:rightskewed_hist}右偏前九高變數的直方圖", fig.height=8----
# 偏態係數高於3的變數
sort(skewValues[skewValues > 3], decreasing = TRUE)
# 取出右偏前九高的變數名稱
highlySkewed <- names(sort(skewValues, decreasing=TRUE))[1:9]
# 成批繪製直方圖
op <- par(mfrow = c(3,3))
invisible(lapply(highlySkewed, function(u)
{hist(segTrainXNC[[u]], main = paste("Histogram of ",
u), xlab = "", cex.main = 0.8)}))
par(op)
## ----echo=FALSE, fig.align="center", fig.cap = "\\label{fig:leftskewed_hist}左偏前九高變數的直方圖", fig.height=8----
symm <- names(sort(skewValues))[1:9]
op <- par(mfrow = c(3,3))
invisible(lapply(symm, function(u) {hist(segTrainXNC[[u]],
main = paste("Histogram of ", u),
xlab = "", cex.main = 0.8)}))
par(op)
## ----warning=FALSE, message=FALSE----------------------------------------
library(caret)
# BC轉換報表(lambda估計值為-0.9)
(Ch1AreaTrans <- BoxCoxTrans(segTrainXNC$AreaCh1))
## ------------------------------------------------------------------------
# 前六筆原始資料
head(segTrainXNC$AreaCh1)
# predict()泛型函數執行BC轉換計算
predict(Ch1AreaTrans, head(segTrainXNC$AreaCh1))
# 自撰BC公式驗證
(head(segTrainXNC$AreaCh1)^(-.9) - 1)/(-.9)
## 2.3.2 屬性萃取之主成份分析 ------------------------------------------------------------------------
# 資料標準化後進行PCA運算
pcaObject <- prcomp(segTrainXNC, center = TRUE, scale. = TRUE)
names(pcaObject)
scale(as.matrix(segTrainXNC)) %*% pcaObject$rotation
scale(as.matrix(segTrainXNC)) %*% pcaObject$rotation ==
pcaObject$x
sum(scale(as.matrix(segTrainXNC)) %*% pcaObject$rotation ==
pcaObject$x)
1009*58
(percentVariance <- pcaObject$sd^2/sum(pcaObject$sd^2)*100)
plot(percentVariance[1:25], xlab = "Principal Component",
ylab = "Proportion of Variance Explained ", type = 'b')
?corrplot
??corrplot
library("animation", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
kmeans.ani()
kmeans.ani()
kmeans.ani()
kmeans.ani()
load("/Volumes/DUAL DRIVE/GoogleDrive_HP/PragmaticBigDataAnalytics/supplements/iSpan/HM/HM.RData")
library("animation", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
kmeans.ani(centers = 5)
kmeans.ani(centers = 5)
kmeans.ani(centers = 3)
kmeans.ani(centers = 5)
teens <- read.csv(file.choose()) # please choose snsdata.csv
library("animation", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
kmeans.ani()
kmeans.ani()
kmeans.ani()
kmeans.ani()
kmeans.ani()
setwd("/Volumes/DUAL DRIVE/GoogleDrive_HP/PragmaticBigDataAnalytics/supplements/MFGdata/TennesseeEastmanProcess")
library(tidyverse)
load('TEP_FaultFree_Testing.RData')
load('TEP_FaultFree_Testing')
install.packages("ForeCA")
data(orange)
install.packages("missMDA")
library(missMDA)
data(orange)
names(orange)
nb <- estim_ncpPCA(orange, ncp.min=0, ncp.max=4)
nb
library("devtools", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install_github("rstudio/reticulate")
load("/Volumes/DUAL DRIVE/Google Drive/MCUT/CTA3/DATA/20220329/separate.RData")
View(process)
View(v202A)
View(v205)
subset(v205[,'V205Co'], Time >= "2021-05-10" & Time <= "2021-06-30")
subset(v205, Time >= "2021-05-10" & Time <= "2021-06-30")
normal <- subset(v205, Time >= "2021-05-10" & Time <= "2021-06-30")
View(normal)
plot(normal)
plot(normal[,'V205Co'])
plot(normal[,'V205Mn'])
plot(normal[,'V205Br'])
View(v205)
View(v205)
question <- subset(v205, Time >= "2021-01-01" & Time <= "2021-05-09")
plot(question[,'V205Co'])
plot(question[,'V205Mn'])
plot(question[,'V205Br'])
normal_pv <- subset(process, Time >= "2021-05-10" & Time <= "2021-06-30")
plot(normal_pv[,'FT_20508.PV'])
question_pv <- subset(process, Time >= "2021-01-01" & Time <= "2021-05-09")
plot(question_pv[,'FT_20508.PV'])
plot(normal_pv[,'FRIC_20416.PV']) # 觸媒比
plot(question_pv[,'FRIC_20416.PV']) # 觸媒比
plot(normal_pv[,'FIC_52103.PV']) # 觸媒比
plot(question_pv[,'FIC_52103.PV']) # Purge流量
library("animation", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
cv.ani()
boot.iid()
sqrt(12)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/opt/anaconda3/bin/python3")
# 先到Spyder中輸入下兩列指令
# import sys
# sys.prefix
# 查看妳/你的Python3路徑'/opt/anaconda3'，再修改use_python()中的路徑
# 或是在終端機/CMD中輸入which python查看路徑
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
load("./data/ticdata.rda")
names(ticdata)
load("./data/ticdata.rda")
names(ticdata)
dat <- read.csv("/Volumes/DUAL\ DRIVE/GoogleDrive_HP/PragmaticBigDataAnalytics_out/supplements/02_supplements_r/caravan-insurance-challenge.csv")
load("./data/ticdata.rda")
setdiff(names(dat), names(ticdata))
dat <- read.csv("/Volumes/DUAL\ DRIVE/GoogleDrive_HP/PragmaticBigDataAnalytics_out/supplements/02_supplements_r/caravan-insurance-challenge.csv")
load("./data/ticdata.rda")
setdiff(names(dat), names(ticdata))
names(dat)
names(ticdata)
dat <- read.csv("/Volumes/DUAL\ DRIVE/GoogleDrive_HP/PragmaticBigDataAnalytics_out/supplements/02_supplements_r/caravan-insurance-challenge.csv")
load("./data/ticdata.rda")
intersect(names(dat), names(ticdata))
setdiff(names(dat), intersect(names(dat), names(ticdata)))
setdiff(names(ticdata), intersect(names(dat), names(ticdata)))
ticdata <- read.csv("/Volumes/DUAL\ DRIVE/GoogleDrive_HP/PragmaticBigDataAnalytics_out/supplements/02_supplements_r/caravan-insurance-challenge.csv")
# load("./data/ticdata.rda")
# intersect(names(dat), names(ticdata)) # 84
# setdiff(names(dat), intersect(names(dat), names(ticdata))) # 3
# setdiff(names(ticdata), intersect(names(dat), names(ticdata))) # 2
# "ORIGIN": real difference; "MOSTYPE" = "STYPE"; "PVRAAUT" = "AAUT"
unique(ticdata$MOSTYPE)
barplot(table(ticdata$MOSTYPE),las=2)
unique(ticdata$MAANTHUI)
hist(ticdata$MAANTHUI)
unique(ticdata$MGEMOMV)
barplot(table(ticdata$MAANTHUI))
unique(ticdata$MGEMLEEF)
barplot(table(ticdata$MGEMLEEF))
unique(ticdata$MOSHOOFD)
barplot(table(ticdata$MOSHOOFD),las=2)
unique(ticdata$MGODRK)
barplot(table(ticdata$MGODRK),las=2)
unique(ticdata$MGODPR)
barplot(table(ticdata$MGODPR),las=2)
unique(ticdata$MGODOV)
barplot(table(ticdata$MGODOV),las=2)
unique(ticdata$MGODGE)
barplot(table(ticdata$MGODGE),las=2)
unique(ticdata$PWAPART)
table(ticdata$CARAVAN) # 9236/586 = 15.76109
## 3.5.3 類別變數視覺化關聯檢驗 ------------------------------------------------------------------------
# 注意類別為'table'的表格形式
str(HairEyeColor)
# 沿著Sex加總Hair-Eye邊際頻次，並變更為Eye-Hair的呈現方式
(tab <- margin.table(HairEyeColor, c(2,1)))
# 載入類別資料視覺化套件，為了呼叫sieve()
library(vcd)
## ----fig.align="center", fig.cap = "\\label{fig:sieve_diagram}眼色-髮色濾網圖"----
# 表格傳入繪製濾網圖
sieve(tab, shade = TRUE)
citation('rpart')
?kmeans
a <- 1:10
sqrt(a)
library(animation)
kmeans.ani(centers = 7)
kmeans.ani(centers = 3)
kmeans.ani(centers = 3)
kmeans.ani(centers = 3)
kmeans.ani(centers = 3)
128*5*5
2^7
6905/(6905+7441+7145)
load("/Volumes/KINGSTON/Google Drive/MCUT/CTA3/分析程式碼回顧/allD0727(20000).RData")
complete.cases(dat)
complete.cases(allD)
sum(complete.cases(allD))
sum(complete.cases(allD[,1:64]))
dat=allD[c(754930:1161010),]  #僅取用數據正常的區段
View(allD)
remain <- function(COL,c1=class(COL), up=F) {
class(COL) -> c1                   #提取資料屬性
as.character(COL) -> x
length(x) -> n
if (up==F)
for (i in c(1:n)) {
ifelse(is.na(x[i]),x[i-1],x[i])->x[i]
}
else
for (i in c(n:1)) {
ifelse(is.na(x[i]),x[i+1],x[i])->x[i]
}
paste('as.',c1,'(x)',sep='') -> t   #做成指令字串
eval(parse(text=t)) -> x            #執行指令字串套回原始資料屬性
return(x)
}
process <- dat[,c(1:64,68:70)]  %>% #製程點位數據組(65:67 V205檢驗數值先拿掉)
filter(!is.na(v2)) %>% #先濾除無製程點位資料之記錄
mutate(Co=remain(Co),   #新觸媒比例（每日一次檢驗數值），故檢驗後數值沿用至下次一檢驗
Mn=remain(Mn),
Br=remain(Br))
library(dplyr)
process <- dat[,c(1:64,68:70)]  %>% #製程點位數據組(65:67 V205檢驗數值先拿掉)
filter(!is.na(v2)) %>% #先濾除無製程點位資料之記錄
mutate(Co=remain(Co),   #新觸媒比例（每日一次檢驗數值），故檢驗後數值沿用至下次一檢驗
Mn=remain(Mn),
Br=remain(Br))
sum(complete.cases(dat[,1:64]))
tmp <- 1:100
runmean(tmp, 10, align="right")
library(caTools) # for runmean()
runmean(tmp, 10, align="right")
mean(2:11)
process[,c(2:64)] = apply(process[,c(2:64)], 2, function(x) runmean(x, 10, align="right"))  #計算移動平均
v205 <- dat[,c(1,65:67)] %>%  #底下先篩每4小時一筆觸媒濃度檢測記錄
filter(as.numeric(gsub(":","",substr(as.character(Time),start=12,stop=16))) %in% c(100,500,900,1300,1700,2100))
View(v205)
#filter(!is.na(V205Co)) #V205觸媒濃度QC檢驗值
rm(dat)
options(scipen=999) #關閉科學記號
library(dplyr)
library(caTools) # for runmean()
library(zoo)
library(tseries)
# 讀入RData格式數據 ----
load(file="D:/MC_AIDS/2022/07/0727/allD0727(20000).RData")
#數據可由雲端取得：
#https://drive.google.com/file/d/1RykJEW6ovnS-0qOcQyBqZ0R9ere60IPq/view?usp=sharing
sum(complete.cases(allD[,1:64])) # 1199969
dat=allD[c(754930:1161010),]  #僅取用數據正常的區段
sum(complete.cases(dat[,1:64])) #點位數據皆完整無缺
#往下填空，考慮將資料屬性帶入並轉回
remain <- function(COL,c1=class(COL), up=F) {
class(COL) -> c1                   #提取資料屬性
as.character(COL) -> x
length(x) -> n
if (up==F)
for (i in c(1:n)) {
ifelse(is.na(x[i]),x[i-1],x[i])->x[i]
}
else
for (i in c(n:1)) {
ifelse(is.na(x[i]),x[i+1],x[i])->x[i]
}
paste('as.',c1,'(x)',sep='') -> t   #做成指令字串
eval(parse(text=t)) -> x            #執行指令字串套回原始資料屬性
return(x)
}
# 將資料拆解 ----
process <- dat[,c(1:64,68:70)]  %>% #製程點位數據組(65:67 V205檢驗數值先拿掉)
filter(!is.na(v2)) %>% #先濾除無製程點位資料之記錄
mutate(Co=remain(Co),   #新觸媒比例（每日一次檢驗數值），故檢驗後數值沿用至下次一檢驗
Mn=remain(Mn),
Br=remain(Br))
sum(complete.cases(process[,1:64])) # 406081
process[,c(2:64)] = apply(process[,c(2:64)], 2, function(x) runmean(x, 10, align="right"))  #計算移動平均
# tmp <- 1:100
# runmean(tmp, 10, align="right")
v205 <- dat[,c(1,65:67)] %>%  #底下先篩每4小時一筆觸媒濃度檢測記錄
filter(as.numeric(gsub(":","",substr(as.character(Time),start=12,stop=16))) %in% c(100,500,900,1300,1700,2100))
load("/Volumes/KINGSTON/Google Drive/MCUT/CTA3/分析程式碼回顧/allD0727(20000).RData")
options(scipen=999) #關閉科學記號
library(dplyr)
library(caTools) # for runmean()
library(zoo)
library(tseries)
# 讀入RData格式數據 ----
# load(file="D:/MC_AIDS/2022/07/0727/allD0727(20000).RData")
load("/Volumes/KINGSTON/Google Drive/MCUT/CTA3/分析程式碼回顧/allD0727(20000).RData")
#數據可由雲端取得：
#https://drive.google.com/file/d/1RykJEW6ovnS-0qOcQyBqZ0R9ere60IPq/view?usp=sharing
sum(complete.cases(allD[,1:64])) # 1199969
dat=allD[c(754930:1161010),]
sum(complete.cases(dat[,1:64])) #點位數據皆完整無缺
#往下填空，考慮將資料屬性帶入並轉回
remain <- function(COL,c1=class(COL), up=F) {
class(COL) -> c1                   #提取資料屬性
as.character(COL) -> x
length(x) -> n
if (up==F)
for (i in c(1:n)) {
ifelse(is.na(x[i]),x[i-1],x[i])->x[i]
}
else
for (i in c(n:1)) {
ifelse(is.na(x[i]),x[i+1],x[i])->x[i]
}
paste('as.',c1,'(x)',sep='') -> t   #做成指令字串
eval(parse(text=t)) -> x            #執行指令字串套回原始資料屬性
return(x)
}
# 將資料拆解 ----
process <- dat[,c(1:64,68:70)]  %>% #製程點位數據組(65:67 V205檢驗數值先拿掉)
filter(!is.na(v2)) %>% #先濾除無製程點位資料之記錄
mutate(Co=remain(Co),   #新觸媒比例（每日一次檢驗數值），故檢驗後數值沿用至下次一檢驗
Mn=remain(Mn),
Br=remain(Br))
sum(complete.cases(process[,1:64])) # 406081
process[,c(2:64)] = apply(process[,c(2:64)], 2, function(x) runmean(x, 10, align="right"))  #計算移動平均
# tmp <- 1:100
# runmean(tmp, 10, align="right")
v205 <- dat[,c(1,65:67)] %>%  #底下先篩每4小時一筆觸媒濃度檢測記錄
filter(as.numeric(gsub(":","",substr(as.character(Time),start=12,stop=16))) %in% c(100,500,900,1300,1700,2100))
#filter(!is.na(V205Co)) #V205觸媒濃度QC檢驗值
View(process)
View(process)
##資料初步統整結果
mm <- merge(process, v205, by="Time", all.x = TRUE) %>%
filter(as.numeric(gsub(":","",substr(as.character(Time),start=16,stop=16)))==0)  # 保留10分鐘一筆記錄(抓分鐘尾巴為0者)
View(process)
View(mm)
outlierP <- function(dat, niqr=3, maN=2){
Q1=as.numeric(quantile(dat,0.25,na.rm=TRUE))
Q3=as.numeric(quantile(dat,0.75,na.rm=TRUE))
L=Q1-niqr*(Q3-Q1)
U=Q3+niqr*(Q3-Q1)
dd=ifelse(dat>U|dat<L, NA, dat)
for (i in c(1:length(dd))) {
if (is.na(dd[i]) & i<(maN+1) ) {
dd[i]=mean(dd[c(-(i-1):maN)+i], na.rm=TRUE )
if (is.nan(dd[i])) {dd[i]=NA}
} else if (is.na(dd[i])) {
dd[i]=mean(dd[c(-maN:maN)+i], na.rm=TRUE )
if (is.nan(dd[i])) {dd[i]=NA}
}
}
dd=remain(dd)
return(dd)
}
#資料再篩選（提供後續數據模流量、觸媒濃度推算）
mm1 <- mm  %>%  dplyr::select(Time, v4, v8, v10, v11, v14, v16, v18, v19, v20, v22, v24, v26,
v30, v32, v34, v39, v40, v44, v45,
v37, v47, v48, v49, v50, v51, v52, v58, v62, Co, Mn, Br,
V205Co, V205Mn, V205Br) %>%
mutate( QQ=ifelse(!is.na(V205Co), 1, 0),
vCo=remain(lag(V205Co)), vMn=remain(lag(V205Mn)), vBr=remain(lag(V205Br)) )  #觸媒濃度檢驗值（取lag()後）沿用至下次一檢驗
?smoothScatter
## 2.2.2 R語言群組與摘要 ------------------------------------------------------------------------
# 知名的鳶尾花資料集
head(iris, 3) # iris.head(n=3) in Python
## ------------------------------------------------------------------------
# 限定環境與結合模型公式符號的aggregate()
aggregate(Sepal.Length ~ Species, data = iris,
FUN = 'length')
?iris
## ----warning=FALSE, message=FALSE----------------------------------------
# 載入R語言知名圖形文法繪圖套件{ggplot2}
library(ggplot2)
# 讀取套件{ggplot2}中的鑽石資料集
data(diamonds)
# 物件類別為"tbl_df"
class(diamonds)
head(diamonds)
aggregate(cbind(price, carat) ~ cut + color,
data = diamonds, "mean")
library(animation)
cv.ani()
cv.ani()
cv.ani()
cv.ani()
cv.ani()
boot.iid()
boot.iid()
setwd("~/GoogleDrive_HP/PragmaticBigDataAnalytics/codes_R")
## 3.3.2 單類模型參數調校 ####
## ----warning=FALSE, message=FALSE----------------------------------------
# 載入R套件與資料集
library(ISLR) # Machine Learning -> Statistical Learning -> Probabilistic Machine Learning
caret
library(caret)
data(Smarket)
createDataPartition(y = Smarket$Direction,
p = 0.75,list = FALSE)
## ------------------------------------------------------------------------
# 校驗集(75%)與(最終)測試集(25%)切分
set.seed(300)
indxCalib <- createDataPartition(y = Smarket$Direction,
p = 0.75,list = FALSE)
calibration <- Smarket[indxCalib, -1] # 註：此例將'Year'變量移除再進行kNN建模較為合理
testing <- Smarket[-indxCalib, -1]
## ------------------------------------------------------------------------
set.seed(400)
par(mfrow=c(1,1))
cv.ani()
# 校驗集重抽樣方法設定
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
# 圖3.15對每一個參數候選值進行訓練與測試
knnFit <- train(Direction ~ ., data = calibration, method = # 注意！傳入calibration而非calibX！所以需要標準化
"knn", trControl = ctrl, preProcess = c("center","scale"),
tuneLength = 20) # 每個k值實驗了3*10(30)次後，再計算各次的正確率(Acccuracy)，然後統計30次的平均值(knnFit$results$Accuracy)與標準差(knnFit$results$AccuracySD)
## ----fig.align="center", fig.cap = "\\label{fig:knn_tuningplot}$k$近鄰法參數調校圖"----
# 重要的參數調校報表
knnFit
(43-5)/2+1
# 各參數候選值更詳細的績效評量概況
knnFit$results
# 圖勝於表，上表前兩欄的折線圖
plot(knnFit)
## ------------------------------------------------------------------------
# 自動以最佳模型(k=13原43)預測最終測試集樣本
knnPredict <- predict(knnFit, newdata = testing) # knnFit.predict(newdata = testing) in Python
knnPredict
# 混淆矩陣、整體分類績效指標與類別相關指標
confusionMatrix(knnPredict, testing$Direction)
knnPredict == testing$Direction
mean(knnPredict == testing$Direction)
## ----fig.align="center", fig.cap = "\\label{fig:knn_rocplot}$k$近鄰分類模型之測試資料操作特性曲線圖"----
# 載入ROC曲線繪製與分析套件
library(pROC)
# 繪製ROC曲線需計算測試資料的類別機率預測值
knnPredict <- predict(knnFit, newdata = testing, type = "prob")
knnROC <- roc(testing$Direction,knnPredict[,"Down"]) # 注意陽性事件定義為"Down"
# 繪製ROC曲線
plot(knnROC, type = "S", print.thres = 0.5)
knn.ani()
cv.ani()
